{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525cb661",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "525cb661",
    "outputId": "4de4e8f5-ffdf-49be-98a3-610b0aeafda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in ./venv/lib/python3.12/site-packages (7.0.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.12/site-packages (from ipykernel) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (9.6.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in ./venv/lib/python3.12/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.12/site-packages (from ipykernel) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in ./venv/lib/python3.12/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in ./venv/lib/python3.12/site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in ./venv/lib/python3.12/site-packages (from ipykernel) (7.1.1)\n",
      "Requirement already satisfied: pyzmq>=25 in ./venv/lib/python3.12/site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in ./venv/lib/python3.12/site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./venv/lib/python3.12/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Installed kernelspec myenv in /root/.local/share/jupyter/kernels/myenv\n",
      "Requirement already satisfied: datasets in /venv/main/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: wandb in /venv/main/lib/python3.12/site-packages (0.22.2)\n",
      "Requirement already satisfied: transformers in /venv/main/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: matplotlib in /venv/main/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: torch-tb-profiler in /venv/main/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /venv/main/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /venv/main/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: click>=8.0.1 in /venv/main/lib/python3.12/site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /venv/main/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /venv/main/lib/python3.12/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /venv/main/lib/python3.12/site-packages (from wandb) (6.33.0)\n",
      "Requirement already satisfied: pydantic<3 in /venv/main/lib/python3.12/site-packages (from wandb) (2.12.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /venv/main/lib/python3.12/site-packages (from wandb) (2.42.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /venv/main/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /venv/main/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /venv/main/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /venv/main/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /venv/main/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /venv/main/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /venv/main/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /venv/main/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: tensorboard!=2.1.0,>=1.15 in /venv/main/lib/python3.12/site-packages (from torch-tb-profiler) (2.20.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /venv/main/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /venv/main/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /venv/main/lib/python3.12/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /venv/main/lib/python3.12/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /venv/main/lib/python3.12/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.9)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /venv/main/lib/python3.12/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /venv/main/lib/python3.12/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /venv/main/lib/python3.12/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /venv/main/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /venv/main/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m venv venv\n",
    "!venv/bin/python -m pip install ipykernel\n",
    "!venv/bin/python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n",
    "%pip install datasets wandb transformers matplotlib torch-tb-profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ANKbTyr50m9f",
   "metadata": {
    "id": "ANKbTyr50m9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "torch.cuda.manual_seed_all(423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "K5CiE3dGktBd",
   "metadata": {
    "id": "K5CiE3dGktBd"
   },
   "outputs": [],
   "source": [
    "project_name = \"hpml-hw2-llm2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4u7Bhja-hrwT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4u7Bhja-hrwT",
    "outputId": "a0cd6ab9-d2e6-458a-9aa5-bfa1c782d5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "xWy8gotth8SB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWy8gotth8SB",
    "outputId": "2ad07fbc-c752-47bd-8b74-664145fa2469"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yMDy5uS927OD",
   "metadata": {
    "id": "yMDy5uS927OD"
   },
   "source": [
    "## C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "IresskYBi_0H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "IresskYBi_0H",
    "outputId": "aa955777-8b53-46a8-a9e9-89222e30fafc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mykmao1515\u001b[0m (\u001b[33mkaimao-columbia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_022324-aaq01c0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/aaq01c0c' target=\"_blank\">bs32_lr1e-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/aaq01c0c' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/aaq01c0c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"hpml-hw2-llm\", name=f\"bs32_lr1e-4\", group = \"Warm Up Experiment\")\n",
    "\n",
    "wandb.config.update({\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_len\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"num_workers\": 2,\n",
    "    \"epochs\": 5,\n",
    "    \"compile_mode\": False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ZsK9vsmTkPpW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsK9vsmTkPpW",
    "outputId": "1ebaa8b3-6f50-4eb0-fb78-368b734158a4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.92it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3095, Train Accuracy: 0.8674, Test Accuracy: 0.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.87it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1738, Train Accuracy: 0.9341, Test Accuracy: 0.8861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.79it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0891, Train Accuracy: 0.9681, Test Accuracy: 0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.84it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0568, Train Accuracy: 0.9804, Test Accuracy: 0.8705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:44<00:00, 17.74it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0385, Train Accuracy: 0.9876, Test Accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  for batch in tqdm(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      inputs = {\n",
    "          \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "          \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          \"labels\": batch[\"labels\"].to(device),\n",
    "      }\n",
    "\n",
    "      outputs = model(**inputs)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      logits = outputs.logits\n",
    "      preds = torch.argmax(logits, dim = 1)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "      correct = (preds == labels).sum().item()\n",
    "      total_correct += correct\n",
    "      total_samples += len(labels)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "  avg_loss = total_loss / len(train_loader)\n",
    "  avg_accuracy = total_correct / total_samples\n",
    "  train_loss.append(avg_loss)\n",
    "  train_accuracy.append(avg_accuracy)\n",
    "\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for batch in test_loader:\n",
    "          inputs = {\n",
    "              \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "              \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          }\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          logits = model(**inputs).logits\n",
    "          preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "          correct += (preds == labels).sum().item()\n",
    "          total += labels.size(0)\n",
    "\n",
    "  accuracy = correct / total\n",
    "  test_accuracy.append(accuracy)\n",
    "\n",
    "  wandb.log({\"train/loss\": train_loss,\n",
    "           \"train/acc\": avg_accuracy,\n",
    "           \"test/acc\": accuracy})\n",
    "  print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LnQpctb_mJnG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "LnQpctb_mJnG",
    "outputId": "1f71f27b-1104-414b-c7da-bcd53cc1defe"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RIoifO1-23Mo",
   "metadata": {
    "id": "RIoifO1-23Mo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sOdFWGecmKdQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "sOdFWGecmKdQ",
    "outputId": "6e92e593-690d-4642-a360-f7ebddc0c3fd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylabel('Value')\n",
    "plt.title('Figure F1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9412e-2451-486c-9052-661c46cf7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss, train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nl2C8CKt4DcD",
   "metadata": {
    "id": "nl2C8CKt4DcD"
   },
   "source": [
    "### Table T1\n",
    "\n",
    "| Epoch | Train Loss | Train Acc | Test Acc |\n",
    "|-------|------------|-----------|----------|\n",
    "|1|0.0317|0.9902|0.8764|\n",
    "|2|0.0246|0.9920|0.8895|\n",
    "|3|0.0211|0.9932|0.8765|\n",
    "|4|0.0178|0.9940|0.8791|\n",
    "|5|0.0173|0.9947|0.8740|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bnMKrcmd56xs",
   "metadata": {
    "id": "bnMKrcmd56xs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gB0_oon5nMFq",
   "metadata": {
    "id": "gB0_oon5nMFq"
   },
   "source": [
    "# C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "kFQIx5zJnNfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "kFQIx5zJnNfa",
    "outputId": "ad0bdcfc-473a-45ab-c768-946d80f0cad4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/acc</td><td>█▇▆▁▁</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/acc</td><td>0.87176</td></tr><tr><td>train/acc</td><td>0.98756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs32_lr1e-4</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/aaq01c0c' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/aaq01c0c</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_022324-aaq01c0c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_023347-ozs8o4f6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/ozs8o4f6' target=\"_blank\">bs32_lr1e-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/ozs8o4f6' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/ozs8o4f6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=project_name, name=f\"bs32_lr1e-4\", group = \"Baseline Timing\")\n",
    "\n",
    "wandb.config.update({\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_len\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"num_workers\": 2,\n",
    "    \"epochs\": 5,\n",
    "    \"compile_mode\": False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ztMRieKn2bQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ztMRieKn2bQ",
    "outputId": "e6768a04-bd08-46c5-8f4d-753446f7c941"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3083, Train Accuracy: 0.8684, Test Accuracy: 0.9026, data_loading time: 1.1752495765686035           compute time: 41.5190954208374 total epoch time: 43.29253888130188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1707, Train Accuracy: 0.9360, Test Accuracy: 0.8947, data_loading time: 1.1852033138275146           compute time: 41.82760190963745 total epoch time: 43.6026086807251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0853, Train Accuracy: 0.9709, Test Accuracy: 0.8808, data_loading time: 1.141286849975586           compute time: 41.886305809020996 total epoch time: 43.60154056549072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0536, Train Accuracy: 0.9812, Test Accuracy: 0.8782, data_loading time: 1.1710238456726074           compute time: 41.90685701370239 total epoch time: 43.64450740814209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0361, Train Accuracy: 0.9872, Test Accuracy: 0.8604, data_loading time: 1.1747565269470215           compute time: 41.91785144805908 total epoch time: 43.67530798912048\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "data_loading_time_arr = []\n",
    "compute_time_arr = []\n",
    "epoch_time_arr = []\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  data_loading_time = 0\n",
    "  training_compute_time = 0\n",
    "  total_epoch_time = 0\n",
    "  start_data_loading = time.time()\n",
    "  start_epoch_time = time.time()\n",
    "  for batch in train_loader:\n",
    "      torch.cuda.synchronize()\n",
    "      end = time.time()\n",
    "      data_loading_time +=  end - start_data_loading\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      inputs = {\n",
    "          \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "          \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          \"labels\": batch[\"labels\"].to(device),\n",
    "      }\n",
    "\n",
    "      start_compute = time.time()\n",
    "      outputs = model(**inputs)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      torch.cuda.synchronize()\n",
    "      end = time.time()\n",
    "      training_compute_time += end - start_compute\n",
    "\n",
    "      logits = outputs.logits\n",
    "      preds = torch.argmax(logits, dim = 1)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "      correct = (preds == labels).sum().item()\n",
    "      total_correct += correct\n",
    "      total_samples += len(labels)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      start_data_loading = time.time()\n",
    "\n",
    "  end = time.time()\n",
    "  total_epoch_time = end - start_epoch_time\n",
    "\n",
    "  avg_loss = total_loss / len(train_loader)\n",
    "  avg_accuracy = total_correct / total_samples\n",
    "  train_loss.append(avg_loss)\n",
    "  train_accuracy.append(avg_accuracy)\n",
    "\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for batch in test_loader:\n",
    "          inputs = {\n",
    "              \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "              \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          }\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          logits = model(**inputs).logits\n",
    "          preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "          correct += (preds == labels).sum().item()\n",
    "          total += labels.size(0)\n",
    "\n",
    "  accuracy = correct / total\n",
    "  test_accuracy.append(accuracy)\n",
    "\n",
    "  wandb.log({\"train/loss\": train_loss,\n",
    "            \"train/acc\": avg_accuracy,\n",
    "            \"test/acc\": accuracy,\n",
    "             \"data-loading time\": data_loading_time,\n",
    "             \"compute time\": training_compute_time,\n",
    "             \"total epoch time\": total_epoch_time})\n",
    "\n",
    "  data_loading_time_arr.append(data_loading_time)\n",
    "  compute_time_arr.append(training_compute_time)\n",
    "  epoch_time_arr.append(total_epoch_time)\n",
    "  print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}, data_loading time: {data_loading_time} \\\n",
    "          compute time: {training_compute_time} total epoch time: {total_epoch_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tSYJy0kk3ZAQ",
   "metadata": {
    "id": "tSYJy0kk3ZAQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁▆▇██</td></tr><tr><td>data-loading time</td><td>▆█▁▆▆</td></tr><tr><td>test/acc</td><td>█▇▄▄▁</td></tr><tr><td>total epoch time</td><td>▁▇▇▇█</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.91785</td></tr><tr><td>data-loading time</td><td>1.17476</td></tr><tr><td>test/acc</td><td>0.86036</td></tr><tr><td>total epoch time</td><td>43.67531</td></tr><tr><td>train/acc</td><td>0.9872</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs32_lr1e-4</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/ozs8o4f6' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/ozs8o4f6</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_023347-ozs8o4f6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "jLNbkkTs3Zj-",
   "metadata": {
    "id": "jLNbkkTs3Zj-"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "epochs = [1, 2, 3, 4, 5]\n",
    "# Stack bars\n",
    "plt.bar(epochs, np.array(compute_time_arr)/np.array(epoch_time_arr), label='Compute', color='blue')\n",
    "plt.bar(epochs, np.array(data_loading_time_arr)/np.array(epoch_time_arr), bottom=np.array(compute_time_arr)/np.array(epoch_time_arr), label='Data Loading', color='skyblue')\n",
    "\n",
    "# 🧾 Annotate total time on top of each bar\n",
    "# for i in range(len(epochs)):\n",
    "#     plt.text(epochs[i], epoch_time_arr[i] + 0.3, f\"{epoch_time_arr[i]:.1f}s\",\n",
    "#              ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Contribution to Total Time (%)')\n",
    "plt.title('Data vs Compute Contribution per Epoch')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de5ea8d2-8720-41de-97ab-07990accc3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1752495765686035, 1.1852033138275146, 1.141286849975586, 1.1710238456726074, 1.1747565269470215] [41.5190954208374, 41.82760190963745, 41.886305809020996, 41.90685701370239, 41.91785144805908] [43.29253888130188, 43.6026086807251, 43.60154056549072, 43.64450740814209, 43.67530798912048]\n"
     ]
    }
   ],
   "source": [
    "print(data_loading_time_arr, compute_time_arr, epoch_time_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heVz9h_S0O_j",
   "metadata": {
    "id": "heVz9h_S0O_j"
   },
   "source": [
    "# C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ylOAPqx30Qkb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ylOAPqx30Qkb",
    "outputId": "7dd7a589-26bf-4f46-f1ec-030b7104228f"
   },
   "outputs": [],
   "source": [
    "avg_epoch_time_arr = []\n",
    "avg_data_time_arr = []\n",
    "\n",
    "def train():\n",
    "  with wandb.init(\n",
    "      group=\"DataLoader Performance\",\n",
    "      config={\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"max_len\": 256, \n",
    "        \"optimizer\": \"AdamW\", \n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"epochs\": 5, \n",
    "        \"compile_mode\": False\n",
    "    }) as run:\n",
    "      config = wandb.config\n",
    "      run.name = f\"lr{wandb.config.learning_rate}_bs{wandb.config.batch_size}_num_workers{config.num_workers}\"\n",
    "      \n",
    "      train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      model_name = \"distilbert-base-uncased\"\n",
    "      model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "      model.to(device)\n",
    "    \n",
    "      optimizer = getattr(optim, config.optimizer)(model.parameters(), lr=config.learning_rate)\n",
    "      print(f\"---------start train: batch_size({config.batch_size}) lr({config.learning_rate}) num_workers({config.num_workers}) optimizer({config.optimizer})-----\")\n",
    "      train_loss = []\n",
    "      train_accuracy = []\n",
    "      test_accuracy = []\n",
    "      data_loading_time_arr = []\n",
    "      compute_time_arr = []\n",
    "      epoch_time_arr = []\n",
    "    \n",
    "      for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        data_loading_time = 0\n",
    "        training_compute_time = 0\n",
    "        total_epoch_time = 0\n",
    "        start_data_loading = time.time()\n",
    "        start_epoch_time = time.time()\n",
    "        for batch in train_loader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            data_loading_time +=  end - start_data_loading\n",
    "\n",
    "            start_compute = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            training_compute_time += end - start_compute\n",
    "    \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim = 1)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += len(labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            start_data_loading = time.time()\n",
    "    \n",
    "        end = time.time()\n",
    "        total_epoch_time = end - start_epoch_time\n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_accuracy = total_correct / total_samples\n",
    "        train_loss.append(avg_loss)\n",
    "        train_accuracy.append(avg_accuracy)\n",
    "    \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "                logits = model(**inputs).logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "        accuracy = correct / total\n",
    "        test_accuracy.append(accuracy)\n",
    "    \n",
    "        wandb.log({\"train/loss\": train_loss,\n",
    "                  \"train/acc\": avg_accuracy,\n",
    "                  \"test/acc\": accuracy,\n",
    "                  \"data-loading time\": data_loading_time,\n",
    "                  \"compute time\": training_compute_time,\n",
    "                  \"total epoch time\": total_epoch_time})\n",
    "    \n",
    "        data_loading_time_arr.append(data_loading_time)\n",
    "        compute_time_arr.append(training_compute_time)\n",
    "        epoch_time_arr.append(total_epoch_time)\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}, data_loading time: {data_loading_time} \\\n",
    "                compute time: {training_compute_time} total epoch time: {total_epoch_time}\")\n",
    "        print(f\"-------------------------------------------------------------------------------\")\n",
    "          \n",
    "      avg_epoch_time_arr.append(sum(epoch_time_arr)/len(epoch_time_arr))\n",
    "      avg_data_time_arr.append(sum(data_loading_time_arr)/len(data_loading_time_arr))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a152793-ac1f-410f-ab52-536c014e43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 3p7k5crm\n",
      "Sweep URL: https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gicy0i6y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_025004-gicy0i6y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/gicy0i6y' target=\"_blank\">giddy-sweep-1</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/gicy0i6y' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/gicy0i6y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(0) optimizer(AdamW)-----\n",
      "Epoch 1/5, Loss: 0.3074, Train Accuracy: 0.8692, Test Accuracy: 0.8851, data_loading time: 3.2625324726104736                 compute time: 41.586782455444336 total epoch time: 45.099496364593506\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 2/5, Loss: 0.1697, Train Accuracy: 0.9364, Test Accuracy: 0.8996, data_loading time: 3.2400829792022705                 compute time: 41.91038393974304 total epoch time: 45.39018106460571\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 3/5, Loss: 0.0900, Train Accuracy: 0.9681, Test Accuracy: 0.8803, data_loading time: 3.2441353797912598                 compute time: 41.98482608795166 total epoch time: 45.45478272438049\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 4/5, Loss: 0.0553, Train Accuracy: 0.9821, Test Accuracy: 0.8635, data_loading time: 3.225752830505371                 compute time: 41.98759627342224 total epoch time: 45.435014963150024\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0458, Train Accuracy: 0.9848, Test Accuracy: 0.8676, data_loading time: 3.2329676151275635                 compute time: 41.983314514160156 total epoch time: 45.4404833316803\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁▇███</td></tr><tr><td>data-loading time</td><td>█▄▄▁▂</td></tr><tr><td>test/acc</td><td>▅█▄▁▂</td></tr><tr><td>total epoch time</td><td>▁▇███</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.98331</td></tr><tr><td>data-loading time</td><td>3.23297</td></tr><tr><td>test/acc</td><td>0.86756</td></tr><tr><td>total epoch time</td><td>45.44048</td></tr><tr><td>train/acc</td><td>0.98484</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0001_bs32</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/gicy0i6y' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/gicy0i6y</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_025004-gicy0i6y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xp3xsqh3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_025534-xp3xsqh3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/xp3xsqh3' target=\"_blank\">clean-sweep-2</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/xp3xsqh3' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/xp3xsqh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3045, Train Accuracy: 0.8700, Test Accuracy: 0.8980, data_loading time: 1.43206787109375                 compute time: 42.13941979408264 total epoch time: 43.948729515075684\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1615, Train Accuracy: 0.9410, Test Accuracy: 0.8802, data_loading time: 1.2880611419677734                 compute time: 42.132593870162964 total epoch time: 43.774178981781006\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0839, Train Accuracy: 0.9710, Test Accuracy: 0.8930, data_loading time: 1.3085389137268066                 compute time: 42.14713144302368 total epoch time: 43.813663482666016\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0534, Train Accuracy: 0.9821, Test Accuracy: 0.8836, data_loading time: 1.2157397270202637                 compute time: 42.06484770774841 total epoch time: 43.611894369125366\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0481, Train Accuracy: 0.9845, Test Accuracy: 0.8762, data_loading time: 1.2825431823730469                 compute time: 42.0499529838562 total epoch time: 43.64691138267517\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▇▇█▂▁</td></tr><tr><td>data-loading time</td><td>█▃▄▁▃</td></tr><tr><td>test/acc</td><td>█▂▆▃▁</td></tr><tr><td>total epoch time</td><td>█▄▅▁▂</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>42.04995</td></tr><tr><td>data-loading time</td><td>1.28254</td></tr><tr><td>test/acc</td><td>0.87616</td></tr><tr><td>total epoch time</td><td>43.64691</td></tr><tr><td>train/acc</td><td>0.98452</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0001_bs32</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/xp3xsqh3' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/xp3xsqh3</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_025534-xp3xsqh3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fv72fcpn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_030048-fv72fcpn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/fv72fcpn' target=\"_blank\">curious-sweep-3</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/fv72fcpn' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/fv72fcpn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(4) optimizer(AdamW)-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3024, Train Accuracy: 0.8732, Test Accuracy: 0.9009, data_loading time: 1.2394201755523682                 compute time: 41.9440553188324 total epoch time: 43.53215169906616\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1699, Train Accuracy: 0.9370, Test Accuracy: 0.8876, data_loading time: 1.2744231224060059                 compute time: 42.048025608062744 total epoch time: 43.652538776397705\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0871, Train Accuracy: 0.9709, Test Accuracy: 0.8801, data_loading time: 1.558837652206421                 compute time: 42.26429533958435 total epoch time: 44.21101760864258\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0548, Train Accuracy: 0.9808, Test Accuracy: 0.8703, data_loading time: 1.8174281120300293                 compute time: 42.46608066558838 total epoch time: 44.67887258529663\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0444, Train Accuracy: 0.9852, Test Accuracy: 0.8741, data_loading time: 1.2845323085784912                 compute time: 42.04469895362854 total epoch time: 43.68988227844238\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁▂▅█▂</td></tr><tr><td>data-loading time</td><td>▁▁▅█▂</td></tr><tr><td>test/acc</td><td>█▅▃▁▂</td></tr><tr><td>total epoch time</td><td>▁▂▅█▂</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>42.0447</td></tr><tr><td>data-loading time</td><td>1.28453</td></tr><tr><td>test/acc</td><td>0.87412</td></tr><tr><td>total epoch time</td><td>43.68988</td></tr><tr><td>train/acc</td><td>0.98524</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0001_bs32</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/fv72fcpn' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/fv72fcpn</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_030048-fv72fcpn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: giqclgi7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_030602-giqclgi7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/giqclgi7' target=\"_blank\">fancy-sweep-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/3p7k5crm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/giqclgi7' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/giqclgi7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(8) optimizer(AdamW)-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3091, Train Accuracy: 0.8666, Test Accuracy: 0.8940, data_loading time: 1.3921043872833252                 compute time: 41.97293710708618 total epoch time: 43.72316646575928\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1647, Train Accuracy: 0.9394, Test Accuracy: 0.8846, data_loading time: 1.4255974292755127                 compute time: 42.07152271270752 total epoch time: 43.8545024394989\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0873, Train Accuracy: 0.9694, Test Accuracy: 0.8858, data_loading time: 1.4303972721099854                 compute time: 42.07430100440979 total epoch time: 43.85521960258484\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0513, Train Accuracy: 0.9830, Test Accuracy: 0.8728, data_loading time: 1.4163362979888916                 compute time: 42.061704874038696 total epoch time: 43.83255934715271\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0443, Train Accuracy: 0.9860, Test Accuracy: 0.8550, data_loading time: 1.4148354530334473                 compute time: 42.051302433013916 total epoch time: 43.81689810752869\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁██▇▆</td></tr><tr><td>data-loading time</td><td>▁▇█▅▅</td></tr><tr><td>test/acc</td><td>█▆▇▄▁</td></tr><tr><td>total epoch time</td><td>▁██▇▆</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>42.0513</td></tr><tr><td>data-loading time</td><td>1.41484</td></tr><tr><td>test/acc</td><td>0.855</td></tr><tr><td>total epoch time</td><td>43.8169</td></tr><tr><td>train/acc</td><td>0.98604</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0001_bs32</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/giqclgi7' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/giqclgi7</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_030602-giqclgi7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'num_workers': {'values': [0, 2, 4, 8]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "wandb.agent(sweep_id, function=train, count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "debd17bb-5c47-4596-85c8-9e8c8d3cd87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "num_workers = [0, 2, 4, 8]\n",
    "plt.plot(num_workers, avg_epoch_time_arr, label='Avg Epoch Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Avg Epoch Time (s)')\n",
    "plt.title('Figure F3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7519de0a-90d0-4061-9251-05710f92e6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.36399168968201, 43.75907554626465, 43.95289258956909, 43.81646919250488] [3.2410942554473876, 1.3053901672363282, 1.4349282741546632, 1.4158541679382324]\n"
     ]
    }
   ],
   "source": [
    "print(avg_epoch_time_arr, avg_data_time_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b708d1",
   "metadata": {},
   "source": [
    "# C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b695c218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs32_lr1e-4</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/0fmkudof' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/0fmkudof</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_060930-0fmkudof/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_061156-lda9fw7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/lda9fw7a' target=\"_blank\">bs32_lr1e-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/lda9fw7a' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/lda9fw7a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config num:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:47<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.3067, Train Accuracy: 0.8695, Test Accuracy: 0.9015\n",
      "\n",
      "===== CPU Profiling =====\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  cudaStreamSynchronize        28.90%       22.115s        28.90%       22.115s       2.357ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B          9384  \n",
      "                                               backward        21.53%       16.479s        21.75%       16.647s      21.287ms       0.000us         0.00%     591.550us       0.756us     -73.31 KB     -73.28 KB   -1638.43 GB   -1638.27 GB           782  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         7.29%        5.579s         7.34%        5.617s       3.587ms       0.000us         0.00%       0.000us       0.000us     195.69 MB     195.50 MB           0 B           0 B          1566  \n",
      "                                                forward         4.79%        3.669s        12.61%        9.647s      12.336ms       0.000us         0.00%       15.294s      19.557ms      73.31 KB      -3.05 KB    1836.28 GB    -381.56 GB           782  \n",
      "                              Optimizer.step#AdamW.step         4.69%        3.586s         7.71%        5.903s       7.548ms       0.000us         0.00%        2.779s       3.554ms         416 B      -3.05 KB     510.83 MB    -195.05 GB           782  \n",
      "                                       cudaLaunchKernel         2.76%        2.116s         2.80%        2.145s       8.673us       0.000us         0.00%     798.937us       0.003us           0 B           0 B           0 B           0 B        247301  \n",
      "                                            aten::addmm         2.66%        2.035s         3.68%        2.814s      47.355us       26.199s        46.90%       26.201s     440.854us           0 B           0 B    1977.68 GB    1977.68 GB         59432  \n",
      "                                               aten::mm         2.58%        1.973s         3.70%        2.829s      47.609us       16.342s        29.25%       16.342s     274.969us           0 B           0 B    1117.21 GB    1117.21 GB         59432  \n",
      "                                        aten::transpose         1.30%     991.595ms         1.78%        1.359s       4.324us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        314364  \n",
      "                                              aten::sum         1.29%     985.262ms         1.93%        1.476s      40.154us     687.082ms         1.23%     688.771ms      18.740us           0 B           0 B       2.41 GB       2.41 GB         36754  \n",
      "                                         cuLaunchKernel         1.23%     939.936ms         1.23%     943.680ms       7.992us       0.000us         0.00%     463.166us       0.004us           0 B           0 B           0 B           0 B        118077  \n",
      "                                            aten::empty         1.17%     896.615ms         1.17%     896.615ms       5.209us       0.000us         0.00%       0.000us       0.000us     469.87 KB     469.87 KB    1646.27 GB    1646.27 GB        172139  \n",
      "                                        cudaMemsetAsync         0.98%     750.617ms         0.98%     750.617ms       5.189us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        144667  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.92%     701.966ms         7.92%        6.064s     204.069us       0.000us         0.00%       16.950s     570.402us           0 B           0 B    -439.20 GB   -1556.54 GB         29716  \n",
      "                                             aten::view         0.88%     670.713ms         0.88%     670.713ms       2.171us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        308890  \n",
      "                                    aten::empty_strided         0.81%     622.397ms         0.81%     622.397ms       5.928us       0.000us         0.00%       0.000us       0.000us           0 B           0 B     372.71 GB     372.71 GB        104996  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.76%     581.743ms         2.12%        1.623s      21.394us       0.000us         0.00%     360.275ms       4.750us           0 B           0 B    -439.45 GB    -439.45 GB         75854  \n",
      "                                                aten::t         0.75%     573.036ms         1.77%        1.357s       7.609us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        178296  \n",
      "                                         AddmmBackward0         0.65%     495.818ms         5.32%        4.070s     136.957us       0.000us         0.00%       16.342s     549.938us           0 B           0 B    1117.21 GB           0 B         29716  \n",
      "                                           aten::linear         0.64%     489.399ms         5.33%        4.080s      68.647us       0.000us         0.00%       26.201s     440.854us           0 B           0 B    1977.68 GB     -25.00 MB         59432  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 76.528s\n",
      "Self CUDA time total: 55.861s\n",
      "\n",
      "\n",
      "===== GPU Profiling =====\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm         2.66%        2.035s         3.68%        2.814s      47.355us       26.199s        46.90%       26.201s     440.854us           0 B           0 B    1977.68 GB    1977.68 GB         59432  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_...         0.00%       0.000us         0.00%       0.000us       0.000us       19.288s        34.53%       19.288s     411.646us           0 B           0 B           0 B           0 B         46856  \n",
      "                                                forward         0.00%       0.000us         0.00%       0.000us       0.000us       16.371s        29.31%       16.371s      20.935ms           0 B           0 B           0 B           0 B           782  \n",
      "                                               aten::mm         2.58%        1.973s         3.70%        2.829s      47.609us       16.342s        29.25%       16.342s     274.969us           0 B           0 B    1117.21 GB    1117.21 GB         59432  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_...         0.00%       0.000us         0.00%       0.000us       0.000us        8.550s        15.31%        8.550s     303.701us           0 B           0 B           0 B           0 B         28152  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x256_...         0.00%       0.000us         0.00%       0.000us       0.000us        6.854s        12.27%        6.854s     725.749us           0 B           0 B           0 B           0 B          9444  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_...         0.00%       0.000us         0.00%       0.000us       0.000us        4.933s         8.83%        4.933s     525.664us           0 B           0 B           0 B           0 B          9384  \n",
      "                    aten::_efficient_attention_backward         0.34%     258.509ms         1.37%        1.050s     223.832us        3.802s         6.81%        3.979s     848.058us           0 B           0 B     329.59 GB    -223.27 GB          4692  \n",
      "fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyT...         0.00%       0.000us         0.00%       0.000us       0.000us        3.802s         6.81%        3.802s     810.373us           0 B           0 B           0 B           0 B          4692  \n",
      "                              Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us        2.817s         5.04%        2.817s       3.602ms           0 B           0 B           0 B           0 B           782  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x64_8...         0.00%       0.000us         0.00%       0.000us       0.000us        2.804s         5.02%        2.804s     149.427us           0 B           0 B           0 B           0 B         18768  \n",
      "                     aten::_efficient_attention_forward         0.23%     179.541ms         0.58%     441.900ms      47.091us        2.332s         4.17%        2.332s     248.496us     146.62 KB       3.66 KB     221.44 GB           0 B          9384  \n",
      "fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us        2.332s         4.17%        2.332s     248.523us           0 B           0 B           0 B           0 B          9383  \n",
      "                                             aten::gelu         0.19%     144.642ms         0.28%     216.680ms      23.090us        1.009s         1.81%        1.009s     107.507us           0 B           0 B     878.91 GB     878.91 GB          9384  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        1.009s         1.81%        1.009s     107.495us           0 B           0 B           0 B           0 B          9384  \n",
      "                                    aten::gelu_backward         0.11%      83.091ms         0.18%     137.218ms      29.245us     727.013ms         1.30%     727.013ms     154.947us           0 B           0 B     439.45 GB     439.45 GB          4692  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     727.013ms         1.30%     727.013ms     154.947us           0 B           0 B           0 B           0 B          4692  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     701.805ms         1.26%     701.805ms      18.697us           0 B           0 B           0 B           0 B         37535  \n",
      "                                              aten::sum         1.29%     985.262ms         1.93%        1.476s      40.154us     687.082ms         1.23%     688.771ms      18.740us           0 B           0 B       2.41 GB       2.41 GB         36754  \n",
      "void at::native::reduce_kernel<128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     590.090ms         1.06%     590.090ms      19.858us           0 B           0 B           0 B           0 B         29716  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 76.528s\n",
      "Self CUDA time total: 55.861s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs32_lr1e-4</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/lda9fw7a' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/lda9fw7a</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_061156-lda9fw7a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251023_062036-bdm9paj4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/bdm9paj4' target=\"_blank\">bs32_lr1e-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/bdm9paj4' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/bdm9paj4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config num:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:49<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.3149, Train Accuracy: 0.8660, Test Accuracy: 0.8974\n",
      "\n",
      "===== CPU Profiling =====\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  cudaStreamSynchronize        32.49%       25.035s        32.49%       25.035s       2.668ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B          9384  \n",
      "                                               backward        21.00%       16.181s        21.19%       16.330s      20.882ms       0.000us         0.00%     599.843us       0.767us     -73.31 KB     -73.30 KB   -1639.57 GB   -1639.41 GB           782  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         8.17%        6.297s         8.22%        6.332s       4.043ms       0.000us         0.00%       0.000us       0.000us     195.69 MB     195.50 MB           0 B           0 B          1566  \n",
      "                              Optimizer.step#AdamW.step         4.31%        3.319s         6.88%        5.301s       6.779ms       0.000us         0.00%        2.782s       3.557ms         416 B      -3.05 KB     511.41 MB    -194.90 GB           782  \n",
      "                                                forward         3.87%        2.981s        10.12%        7.795s       9.968ms       0.000us         0.00%       15.288s      19.550ms      73.31 KB      -3.05 KB    1835.84 GB    -382.18 GB           782  \n",
      "                                               aten::mm         2.54%        1.955s         3.64%        2.808s      47.240us       16.310s        29.21%       16.310s     274.424us           0 B           0 B    1116.08 GB    1116.08 GB         59432  \n",
      "                                       cudaLaunchKernel         2.52%        1.945s         2.57%        1.981s       8.012us       0.000us         0.00%     801.697us       0.003us           0 B           0 B           0 B           0 B        247301  \n",
      "                                            aten::addmm         2.23%        1.715s         3.08%        2.377s      39.989us       26.220s        46.96%       26.223s     441.220us           0 B           0 B    1977.68 GB    1977.68 GB         59432  \n",
      "                                              aten::sum         1.26%     969.475ms         1.88%        1.450s      39.447us     686.954ms         1.23%     688.640ms      18.736us           0 B           0 B       2.41 GB       2.41 GB         36754  \n",
      "                                        aten::transpose         1.22%     938.797ms         1.66%        1.278s       4.065us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        314364  \n",
      "                                         cuLaunchKernel         1.12%     863.095ms         1.13%     868.008ms       7.351us       0.000us         0.00%     463.392us       0.004us           0 B           0 B           0 B           0 B        118077  \n",
      "                                            aten::empty         1.07%     826.792ms         1.07%     826.792ms       4.803us       0.000us         0.00%       0.000us       0.000us     471.18 KB     471.18 KB    1645.82 GB    1645.82 GB        172139  \n",
      "                                        cudaMemsetAsync         0.92%     707.897ms         0.92%     707.897ms       4.893us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        144667  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.89%     682.728ms         7.76%        5.982s     201.321us       0.000us         0.00%       16.918s     569.321us           0 B           0 B    -440.34 GB   -1556.54 GB         29716  \n",
      "                                             aten::view         0.77%     591.821ms         0.77%     591.821ms       1.916us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        308890  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.74%     571.254ms         2.06%        1.588s      20.935us       0.000us         0.00%     359.692ms       4.742us           0 B           0 B    -439.45 GB    -439.45 GB         75854  \n",
      "                                                aten::t         0.72%     551.090ms         1.68%        1.297s       7.276us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        178296  \n",
      "                                    aten::empty_strided         0.65%     497.416ms         0.65%     497.416ms       4.737us       0.000us         0.00%       0.000us       0.000us           0 B           0 B     372.07 GB     372.07 GB        104996  \n",
      "                                         AddmmBackward0         0.63%     486.042ms         5.23%        4.027s     135.510us       0.000us         0.00%       16.310s     548.848us           0 B           0 B    1116.08 GB           0 B         29716  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.60%     460.694ms         1.46%        1.123s      13.805us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B         81328  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 77.057s\n",
      "Self CUDA time total: 55.841s\n",
      "\n",
      "\n",
      "===== GPU Profiling =====\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm         2.23%        1.715s         3.08%        2.377s      39.989us       26.220s        46.96%       26.223s     441.220us           0 B           0 B    1977.68 GB    1977.68 GB         59432  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_...         0.00%       0.000us         0.00%       0.000us       0.000us       19.316s        34.59%       19.316s     412.362us           0 B           0 B           0 B           0 B         46842  \n",
      "                                               aten::mm         2.54%        1.955s         3.64%        2.808s      47.240us       16.310s        29.21%       16.310s     274.424us           0 B           0 B    1116.08 GB    1116.08 GB         59432  \n",
      "                                                forward         0.00%       0.000us         0.00%       0.000us       0.000us       16.197s        29.01%       16.197s      20.712ms           0 B           0 B           0 B           0 B           782  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_...         0.00%       0.000us         0.00%       0.000us       0.000us        8.539s        15.29%        8.539s     303.305us           0 B           0 B           0 B           0 B         28152  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x256_...         0.00%       0.000us         0.00%       0.000us       0.000us        6.847s        12.26%        6.847s     725.189us           0 B           0 B           0 B           0 B          9441  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_...         0.00%       0.000us         0.00%       0.000us       0.000us        4.917s         8.80%        4.917s     523.942us           0 B           0 B           0 B           0 B          9384  \n",
      "                    aten::_efficient_attention_backward         0.33%     253.535ms         1.34%        1.036s     220.874us        3.797s         6.80%        3.973s     846.793us           0 B           0 B     329.59 GB    -223.27 GB          4692  \n",
      "fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyT...         0.00%       0.000us         0.00%       0.000us       0.000us        3.797s         6.80%        3.797s     809.313us           0 B           0 B           0 B           0 B          4692  \n",
      "                              Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us        3.135s         5.61%        3.135s       4.009ms           0 B           0 B           0 B           0 B           782  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x64_8...         0.00%       0.000us         0.00%       0.000us       0.000us        2.798s         5.01%        2.798s     149.095us           0 B           0 B           0 B           0 B         18768  \n",
      "                     aten::_efficient_attention_forward         0.20%     153.980ms         0.49%     379.937ms      40.488us        2.329s         4.17%        2.329s     248.199us     146.62 KB       2.45 KB     221.44 GB           0 B          9384  \n",
      "fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us        2.329s         4.17%        2.329s     248.279us           0 B           0 B           0 B           0 B          9381  \n",
      "                                             aten::gelu         0.16%     121.280ms         0.24%     182.426ms      19.440us        1.010s         1.81%        1.010s     107.624us           0 B           0 B     878.91 GB     878.91 GB          9384  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        1.010s         1.81%        1.010s     107.647us           0 B           0 B           0 B           0 B          9381  \n",
      "                                    aten::gelu_backward         0.11%      81.089ms         0.17%     134.253ms      28.613us     726.345ms         1.30%     726.345ms     154.805us           0 B           0 B     439.45 GB     439.45 GB          4692  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     726.345ms         1.30%     726.345ms     154.805us           0 B           0 B           0 B           0 B          4692  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     700.174ms         1.25%     700.174ms      18.656us           0 B           0 B           0 B           0 B         37530  \n",
      "                                              aten::sum         1.26%     969.475ms         1.88%        1.450s      39.447us     686.954ms         1.23%     688.640ms      18.736us           0 B           0 B       2.41 GB       2.41 GB         36754  \n",
      "void at::native::reduce_kernel<128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     589.879ms         1.06%     589.879ms      19.851us           0 B           0 B           0 B           0 B         29716  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 77.057s\n",
      "Self CUDA time total: 55.841s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "for num_workers in [1,2]:\n",
    "    wandb.init(project=\"hpml-hw2-llm3\", name=f\"bs32_lr1e-4num_works{num_workers}\", group = \"PyTorch Profiler\")\n",
    "\n",
    "    wandb.config.update({\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"max_len\": 256,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 1e-4,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"num_workers\": num_workers,\n",
    "        \"epochs\": 1,\n",
    "        \"compile_mode\": False\n",
    "    })\n",
    "    config = wandb.config\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "    \n",
    "    train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "    test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "    print(f\"config num:{config.num_workers}\")\n",
    "    \n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    with profiler.profile(\n",
    "        activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    "        # on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./profiler_logs\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            for batch in tqdm(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                with profiler.record_function(\"data_loading\"):\n",
    "                    inputs = {\n",
    "                        \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                        \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                        \"labels\": batch[\"labels\"].to(device),\n",
    "                    }\n",
    "    \n",
    "                with profiler.record_function(\"forward\"):\n",
    "                    outputs = model(**inputs)\n",
    "    \n",
    "                with profiler.record_function(\"backward\"):\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "    \n",
    "                with profiler.record_function(\"optimizer\"):\n",
    "                    optimizer.step()\n",
    "    \n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim = 1)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                correct = (preds == labels).sum().item()\n",
    "                total_correct += correct\n",
    "                total_samples += len(labels)\n",
    "                total_loss += loss.item()\n",
    "    \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            avg_accuracy = total_correct / total_samples\n",
    "            train_loss.append(avg_loss)\n",
    "            train_accuracy.append(avg_accuracy)\n",
    "    \n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    inputs = {\n",
    "                        \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                        \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                    }\n",
    "                    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "                    logits = model(**inputs).logits\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "    \n",
    "            accuracy = correct / total\n",
    "            test_accuracy.append(accuracy)\n",
    "            print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            prof.step()\n",
    "    print(\"\\n===== CPU Profiling =====\")\n",
    "\n",
    "    # print(prof.is_active(), prof.profiler)\n",
    "    torch.cuda.synchronize()\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cpu_time_total\",  # or \"cpu_time_total\"\n",
    "        row_limit=20\n",
    "    ))\n",
    "    \n",
    "    # ✅ Print GPU-focused results\n",
    "    print(\"\\n===== GPU Profiling =====\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",  # or \"cuda_time_total\"\n",
    "        row_limit=20\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41f0e58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FunctionEventAvg' object has no attribute 'cuda_time_total'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m evt \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdata_loading\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evt.key \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evt.key \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evt.key \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evt.key:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevt.key\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m20s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m CPU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevt.cpu_time_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m µs  CUDA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mevt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda_time_total\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m µs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'FunctionEventAvg' object has no attribute 'cuda_time_total'"
     ]
    }
   ],
   "source": [
    "Epoch 1/1, Loss: 0.3067, Train Accuracy: 0.8695, Test Accuracy: 0.9015\n",
    "\n",
    "===== CPU Profiling =====\n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                  cudaStreamSynchronize        28.90%       22.115s        28.90%       22.115s       2.357ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B          9384  \n",
    "                                               backward        21.53%       16.479s        21.75%       16.647s      21.287ms       0.000us         0.00%     591.550us       0.756us     -73.31 KB     -73.28 KB   -1638.43 GB   -1638.27 GB           782  \n",
    "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         7.29%        5.579s         7.34%        5.617s       3.587ms       0.000us         0.00%       0.000us       0.000us     195.69 MB     195.50 MB           0 B           0 B          1566  \n",
    "                                                forward         4.79%        3.669s        12.61%        9.647s      12.336ms       0.000us         0.00%       15.294s      19.557ms      73.31 KB      -3.05 KB    1836.28 GB    -381.56 GB           782  \n",
    "                              Optimizer.step#AdamW.step         4.69%        3.586s         7.71%        5.903s       7.548ms       0.000us         0.00%        2.779s       3.554ms         416 B      -3.05 KB     510.83 MB    -195.05 GB           782  \n",
    "                                       cudaLaunchKernel         2.76%        2.116s         2.80%        2.145s       8.673us       0.000us         0.00%     798.937us       0.003us           0 B           0 B           0 B           0 B        247301  \n",
    "                                            aten::addmm         2.66%        2.035s         3.68%        2.814s      47.355us       26.199s        46.90%       26.201s     440.854us           0 B           0 B    1977.68 GB    1977.68 GB         59432  \n",
    "                                               aten::mm         2.58%        1.973s         3.70%        2.829s      47.609us       16.342s        29.25%       16.342s     274.969us           0 B           0 B    1117.21 GB    1117.21 GB         59432  \n",
    "                                        aten::transpose         1.30%     991.595ms         1.78%        1.359s       4.324us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        314364  \n",
    "                                              aten::sum         1.29%     985.262ms         1.93%        1.476s      40.154us     687.082ms         1.23%     688.771ms      18.740us           0 B           0 B       2.41 GB       2.41 GB         36754  \n",
    "                                         cuLaunchKernel         1.23%     939.936ms         1.23%     943.680ms       7.992us       0.000us         0.00%     463.166us       0.004us           0 B           0 B           0 B           0 B        118077  \n",
    "                                            aten::empty         1.17%     896.615ms         1.17%     896.615ms       5.209us       0.000us         0.00%       0.000us       0.000us     469.87 KB     469.87 KB    1646.27 GB    1646.27 GB        172139  \n",
    "                                        cudaMemsetAsync         0.98%     750.617ms         0.98%     750.617ms       5.189us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        144667  \n",
    "    autograd::engine::evaluate_function: AddmmBackward0         0.92%     701.966ms         7.92%        6.064s     204.069us       0.000us         0.00%       16.950s     570.402us           0 B           0 B    -439.20 GB   -1556.54 GB         29716  \n",
    "                                             aten::view         0.88%     670.713ms         0.88%     670.713ms       2.171us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        308890  \n",
    "                                    aten::empty_strided         0.81%     622.397ms         0.81%     622.397ms       5.928us       0.000us         0.00%       0.000us       0.000us           0 B           0 B     372.71 GB     372.71 GB        104996  \n",
    "     autograd::engine::evaluate_function: ViewBackward0         0.76%     581.743ms         2.12%        1.623s      21.394us       0.000us         0.00%     360.275ms       4.750us           0 B           0 B    -439.45 GB    -439.45 GB         75854  \n",
    "                                                aten::t         0.75%     573.036ms         1.77%        1.357s       7.609us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B        178296  \n",
    "                                         AddmmBackward0         0.65%     495.818ms         5.32%        4.070s     136.957us       0.000us         0.00%       16.342s     549.938us           0 B           0 B    1117.21 GB           0 B         29716  \n",
    "                                           aten::linear         0.64%     489.399ms         5.33%        4.080s      68.647us       0.000us         0.00%       26.201s     440.854us           0 B           0 B    1977.68 GB     -25.00 MB         59432  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 76.528s\n",
    "Self CUDA time total: 55.861s\n",
    "\n",
    "\n",
    "===== GPU Profiling =====\n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                            aten::addmm         2.66%        2.035s         3.68%        2.814s      47.355us       26.199s        46.90%       26.201s     440.854us           0 B           0 B    1977.68 GB    1977.68 GB         59432  \n",
    "void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_...         0.00%       0.000us         0.00%       0.000us       0.000us       19.288s        34.53%       19.288s     411.646us           0 B           0 B           0 B           0 B         46856  \n",
    "                                                forward         0.00%       0.000us         0.00%       0.000us       0.000us       16.371s        29.31%       16.371s      20.935ms           0 B           0 B           0 B           0 B           782  \n",
    "                                               aten::mm         2.58%        1.973s         3.70%        2.829s      47.609us       16.342s        29.25%       16.342s     274.969us           0 B           0 B    1117.21 GB    1117.21 GB         59432  \n",
    "void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_...         0.00%       0.000us         0.00%       0.000us       0.000us        8.550s        15.31%        8.550s     303.701us           0 B           0 B           0 B           0 B         28152  \n",
    "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x256_...         0.00%       0.000us         0.00%       0.000us       0.000us        6.854s        12.27%        6.854s     725.749us           0 B           0 B           0 B           0 B          9444  \n",
    "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_...         0.00%       0.000us         0.00%       0.000us       0.000us        4.933s         8.83%        4.933s     525.664us           0 B           0 B           0 B           0 B          9384  \n",
    "                    aten::_efficient_attention_backward         0.34%     258.509ms         1.37%        1.050s     223.832us        3.802s         6.81%        3.979s     848.058us           0 B           0 B     329.59 GB    -223.27 GB          4692  \n",
    "fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyT...         0.00%       0.000us         0.00%       0.000us       0.000us        3.802s         6.81%        3.802s     810.373us           0 B           0 B           0 B           0 B          4692  \n",
    "                              Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us        2.817s         5.04%        2.817s       3.602ms           0 B           0 B           0 B           0 B           782  \n",
    "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x64_8...         0.00%       0.000us         0.00%       0.000us       0.000us        2.804s         5.02%        2.804s     149.427us           0 B           0 B           0 B           0 B         18768  \n",
    "                     aten::_efficient_attention_forward         0.23%     179.541ms         0.58%     441.900ms      47.091us        2.332s         4.17%        2.332s     248.496us     146.62 KB       3.66 KB     221.44 GB           0 B          9384  \n",
    "fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us        2.332s         4.17%        2.332s     248.523us           0 B           0 B           0 B           0 B          9383  \n",
    "                                             aten::gelu         0.19%     144.642ms         0.28%     216.680ms      23.090us        1.009s         1.81%        1.009s     107.507us           0 B           0 B     878.91 GB     878.91 GB          9384  \n",
    "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        1.009s         1.81%        1.009s     107.495us           0 B           0 B           0 B           0 B          9384  \n",
    "                                    aten::gelu_backward         0.11%      83.091ms         0.18%     137.218ms      29.245us     727.013ms         1.30%     727.013ms     154.947us           0 B           0 B     439.45 GB     439.45 GB          4692  \n",
    "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     727.013ms         1.30%     727.013ms     154.947us           0 B           0 B           0 B           0 B          4692  \n",
    "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     701.805ms         1.26%     701.805ms      18.697us           0 B           0 B           0 B           0 B         37535  \n",
    "                                              aten::sum         1.29%     985.262ms         1.93%        1.476s      40.154us     687.082ms         1.23%     688.771ms      18.740us           0 B           0 B       2.41 GB       2.41 GB         36754  \n",
    "void at::native::reduce_kernel<128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     590.090ms         1.06%     590.090ms      19.858us           0 B           0 B           0 B           0 B         29716  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 76.528s\n",
    "Self CUDA time total: 55.861s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762bd39-4758-4888-ad32-8920ced06d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb37d7f2-2ab1-4a21-9f2c-67217a11135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9992b8-0577-4d7d-a668-4620dea914c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis '}' does not match opening parenthesis '(' (2808512440.py, line 114)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mavg_epoch_time.append({\"lr\":config.lr, \"batch_size\":config.batch_size, \"avg_epoch_time\":sum(epoch_time_arr)/len(epoch_time_arr})\u001b[39m\n                                                                                                                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m closing parenthesis '}' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "avg_epoch_time = []\n",
    "test_accuracy = []\n",
    "def train_C6():\n",
    "  with wandb.init(\n",
    "    group=\"Hyperparameter Sensitivity\",\n",
    "    config={\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"max_len\": 256, \n",
    "        \"optimizer\": \"AdamW\", \n",
    "        \"num_workers\": 2,\n",
    "        \"epochs\": 5, \n",
    "        \"compile_mode\": False\n",
    "    }) as run:\n",
    "        \n",
    "      train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      model_name = \"distilbert-base-uncased\"\n",
    "      model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "      model.to(device)\n",
    "    \n",
    "      optimizer = getattr(optim, config.optimizer.capitalize())(model.parameters(), lr=config.learning_rate)\n",
    "      print(f\"---------start train: batch_size({config.batch_size}) lr({config.learning_rate}) num_workers({config.num_workers}) optimizer({config.optimizer})-----\")\n",
    "      train_loss = []\n",
    "      train_accuracy = []\n",
    "      test_accuracy = []\n",
    "      data_loading_time_arr = []\n",
    "      compute_time_arr = []\n",
    "      epoch_time_arr = []\n",
    "    \n",
    "      for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        data_loading_time = 0\n",
    "        training_compute_time = 0\n",
    "        total_epoch_time = 0\n",
    "        start_data_loading = time.time()\n",
    "        start_epoch_time = time.time()\n",
    "        for batch in train_loader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            data_loading_time +=  end - start_data_loading\n",
    "\n",
    "            start_compute = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            training_compute_time += end - start_compute\n",
    "    \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim = 1)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += len(labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            start_data_loading = time.time()\n",
    "    \n",
    "        end = time.time()\n",
    "        total_epoch_time = end - start_epoch_time\n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_accuracy = total_correct / total_samples\n",
    "        train_loss.append(avg_loss)\n",
    "        train_accuracy.append(avg_accuracy)\n",
    "    \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "                logits = model(**inputs).logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "        accuracy = correct / total\n",
    "        test_accuracy.append(accuracy)\n",
    "    \n",
    "        wandb.log({\"train/loss\": train_loss,\n",
    "                  \"train/acc\": avg_accuracy,\n",
    "                  \"test/acc\": accuracy,\n",
    "                  \"data-loading time\": data_loading_time,\n",
    "                  \"compute time\": training_compute_time,\n",
    "                  \"total epoch time\": total_epoch_time})\n",
    "    \n",
    "        data_loading_time_arr.append(data_loading_time)\n",
    "        compute_time_arr.append(training_compute_time)\n",
    "        epoch_time_arr.append(total_epoch_time)\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}, data_loading time: {data_loading_time} \\\n",
    "                compute time: {training_compute_time} total epoch time: {total_epoch_time}\")\n",
    "        print(f\"-------------------------------------------------------------------------------\")\n",
    "        \n",
    "      avg_epoch_time.append({\"lr\":config.lr, \"batch_size\":config.batch_size, \"avg_epoch_time\":sum(epoch_time_arr)/len(epoch_time_arr)})\n",
    "\n",
    "      model.eval()\n",
    "      correct = 0\n",
    "      total = 0\n",
    "    \n",
    "      with torch.no_grad():\n",
    "          for batch in test_loader:\n",
    "              inputs = {\n",
    "                  \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                  \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "              }\n",
    "              labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "              logits = model(**inputs).logits\n",
    "              preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "              correct += (preds == labels).sum().item()\n",
    "              total += labels.size(0)\n",
    "    \n",
    "      accuracy = correct / total\n",
    "      test_accuracy.append({\"lr\":config.lr, \"batch_size\":config.batch_size, \"accuracy\":accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61631ae3-18ea-48fd-9991-72fae38a0b36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: y2g0jplp\n",
      "Sweep URL: https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: if0a1s74 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_043735-if0a1s74</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74' target=\"_blank\">prime-sweep-1</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(16) lr(5e-05) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.2927, Train Accuracy: 0.8786, Test Accuracy: 0.8994, data_loading time: 5.044431447982788                 compute time: 46.83597779273987 total epoch time: 56.109216928482056\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1575, Train Accuracy: 0.9416, Test Accuracy: 0.9047, data_loading time: 5.043533086776733                 compute time: 47.03125500679016 total epoch time: 56.2285680770874\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>█▁</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>47.03126</td></tr><tr><td>data-loading time</td><td>5.04353</td></tr><tr><td>test/acc</td><td>0.90472</td></tr><tr><td>total epoch time</td><td>56.22857</td></tr><tr><td>train/acc</td><td>0.94164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-sweep-1</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_043735-if0a1s74/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c4bfiq97 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044023-c4bfiq97</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97' target=\"_blank\">snowy-sweep-2</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(16) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.3244, Train Accuracy: 0.8612, Test Accuracy: 0.8822, data_loading time: 5.050256013870239                 compute time: 47.20144319534302 total epoch time: 56.59880566596985\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1842, Train Accuracy: 0.9322, Test Accuracy: 0.8745, data_loading time: 5.0730016231536865                 compute time: 47.05021905899048 total epoch time: 56.21663761138916\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>█▁</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>█▁</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>47.05022</td></tr><tr><td>data-loading time</td><td>5.073</td></tr><tr><td>test/acc</td><td>0.87452</td></tr><tr><td>total epoch time</td><td>56.21664</td></tr><tr><td>train/acc</td><td>0.93224</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-2</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044023-c4bfiq97/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: apja8pbi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044314-apja8pbi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi' target=\"_blank\">apricot-sweep-3</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(16) lr(0.0005) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.6958, Train Accuracy: 0.5016, Test Accuracy: 0.5000, data_loading time: 4.986021041870117                 compute time: 47.02136778831482 total epoch time: 56.25733828544617\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.6936, Train Accuracy: 0.4960, Test Accuracy: 0.5000, data_loading time: 4.996425151824951                 compute time: 46.96302270889282 total epoch time: 56.07941198348999\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>█▁</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁▁</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>46.96302</td></tr><tr><td>data-loading time</td><td>4.99643</td></tr><tr><td>test/acc</td><td>0.5</td></tr><tr><td>total epoch time</td><td>56.07941</td></tr><tr><td>train/acc</td><td>0.49596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-sweep-3</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044314-apja8pbi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: soqd5vn4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044602-soqd5vn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4' target=\"_blank\">efficient-sweep-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(5e-05) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.2873, Train Accuracy: 0.8787, Test Accuracy: 0.8747, data_loading time: 4.104927062988281                 compute time: 41.81464457511902 total epoch time: 50.214582443237305\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1558, Train Accuracy: 0.9434, Test Accuracy: 0.9068, data_loading time: 4.095701456069946                 compute time: 41.877474308013916 total epoch time: 50.101362228393555\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>█▁</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.87747</td></tr><tr><td>data-loading time</td><td>4.0957</td></tr><tr><td>test/acc</td><td>0.9068</td></tr><tr><td>total epoch time</td><td>50.10136</td></tr><tr><td>train/acc</td><td>0.9434</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-4</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044602-soqd5vn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a8v7clkd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044839-a8v7clkd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd' target=\"_blank\">vibrant-sweep-5</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.3021, Train Accuracy: 0.8704, Test Accuracy: 0.8973, data_loading time: 4.023552179336548                 compute time: 41.88974690437317 total epoch time: 50.070762634277344\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1693, Train Accuracy: 0.9370, Test Accuracy: 0.8988, data_loading time: 4.086701393127441                 compute time: 42.00715637207031 total epoch time: 50.11471486091614\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>42.00716</td></tr><tr><td>data-loading time</td><td>4.0867</td></tr><tr><td>test/acc</td><td>0.8988</td></tr><tr><td>total epoch time</td><td>50.11471</td></tr><tr><td>train/acc</td><td>0.93696</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sweep-5</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044839-a8v7clkd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7yt0t72f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045113-7yt0t72f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f' target=\"_blank\">desert-sweep-6</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0005) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.6983, Train Accuracy: 0.5016, Test Accuracy: 0.5000, data_loading time: 4.027059078216553                 compute time: 41.73872375488281 total epoch time: 50.55894684791565\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.6932, Train Accuracy: 0.4984, Test Accuracy: 0.5000, data_loading time: 4.3440101146698                 compute time: 41.72826957702637 total epoch time: 50.1086859703064\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>█▁</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁▁</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.72827</td></tr><tr><td>data-loading time</td><td>4.34401</td></tr><tr><td>test/acc</td><td>0.5</td></tr><tr><td>total epoch time</td><td>50.10869</td></tr><tr><td>train/acc</td><td>0.49836</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">desert-sweep-6</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_045113-7yt0t72f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ehbzvjpl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045344-ehbzvjpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl' target=\"_blank\">stellar-sweep-7</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(64) lr(5e-05) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.2931, Train Accuracy: 0.8748, Test Accuracy: 0.9079, data_loading time: 3.631897449493408                 compute time: 40.23127055168152 total epoch time: 47.82063055038452\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1632, Train Accuracy: 0.9385, Test Accuracy: 0.9058, data_loading time: 3.6717135906219482                 compute time: 40.30282783508301 total epoch time: 48.06329941749573\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>█▁</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>40.30283</td></tr><tr><td>data-loading time</td><td>3.67171</td></tr><tr><td>test/acc</td><td>0.90584</td></tr><tr><td>total epoch time</td><td>48.0633</td></tr><tr><td>train/acc</td><td>0.93848</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-7</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_045344-ehbzvjpl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 52db4wm9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045612-52db4wm9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9' target=\"_blank\">polar-sweep-8</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(64) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.3137, Train Accuracy: 0.8618, Test Accuracy: 0.8916, data_loading time: 3.6058061122894287                 compute time: 40.00300621986389 total epoch time: 47.626259088516235\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1613, Train Accuracy: 0.9400, Test Accuracy: 0.9052, data_loading time: 3.7015445232391357                 compute time: 40.06726408004761 total epoch time: 47.8132266998291\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>40.06726</td></tr><tr><td>data-loading time</td><td>3.70154</td></tr><tr><td>test/acc</td><td>0.90516</td></tr><tr><td>total epoch time</td><td>47.81323</td></tr><tr><td>train/acc</td><td>0.93996</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-8</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_045612-52db4wm9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9fm34824 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045835-9fm34824</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/9fm34824' target=\"_blank\">eager-sweep-9</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/9fm34824' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/9fm34824</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(64) lr(0.0005) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.6971, Train Accuracy: 0.5002, Test Accuracy: 0.5000, data_loading time: 3.5461485385894775                 compute time: 39.772443771362305 total epoch time: 47.27660632133484\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1984/2333432887.py\", line 36, in train\n",
      "    for batch in train_loader:\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1492, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1454, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1285, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_1984/2333432887.py\", line 36, in train\n",
      "    for batch in train_loader:\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1492, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1454, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1285, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'learning_rate': {'values': [5e-5, 1e-4, 5e-4]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "wandb.agent(sweep_id, function=train_C6, count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee77f-1105-400a-944d-e071a4fac36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_epoch_time = []\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "def train_C7():\n",
    "  with wandb.init(\n",
    "    group=\"Hyperparameter Sensitivity\",\n",
    "    config={\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"max_len\": 256, \n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_workers\": 2,\n",
    "        \"epochs\": 5, \n",
    "        \"compile_mode\": False\n",
    "    }) as run:\n",
    "        \n",
    "      train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      model_name = \"distilbert-base-uncased\"\n",
    "      model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "      model.to(device)\n",
    "\n",
    "      model_train_accuracy = 0\n",
    "    \n",
    "      optimizer = getattr(optim, config.optimizer.capitalize())(model.parameters(), lr=config.learning_rate)\n",
    "      print(f\"---------start train: batch_size({config.batch_size}) lr({config.learning_rate}) num_workers({config.num_workers}) optimizer({config.optimizer})-----\")\n",
    "      train_loss = []\n",
    "      train_accuracy = []\n",
    "      test_accuracy = []\n",
    "      data_loading_time_arr = []\n",
    "      compute_time_arr = []\n",
    "      epoch_time_arr = []\n",
    "    \n",
    "      for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        data_loading_time = 0\n",
    "        training_compute_time = 0\n",
    "        total_epoch_time = 0\n",
    "        start_data_loading = time.time()\n",
    "        start_epoch_time = time.time()\n",
    "        for batch in train_loader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            data_loading_time +=  end - start_data_loading\n",
    "\n",
    "            start_compute = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            training_compute_time += end - start_compute\n",
    "    \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim = 1)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += len(labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            start_data_loading = time.time()\n",
    "    \n",
    "        end = time.time()\n",
    "        total_epoch_time = end - start_epoch_time\n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_accuracy = total_correct / total_samples\n",
    "        train_loss.append(avg_loss)\n",
    "        train_accuracy.append(avg_accuracy)\n",
    "    \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "                logits = model(**inputs).logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "        accuracy = correct / total\n",
    "        test_accuracy.append(accuracy)\n",
    "    \n",
    "        wandb.log({\"train/loss\": train_loss,\n",
    "                  \"train/acc\": avg_accuracy,\n",
    "                  \"test/acc\": accuracy,\n",
    "                  \"data-loading time\": data_loading_time,\n",
    "                  \"compute time\": training_compute_time,\n",
    "                  \"total epoch time\": total_epoch_time})\n",
    "    \n",
    "        data_loading_time_arr.append(data_loading_time)\n",
    "        compute_time_arr.append(training_compute_time)\n",
    "        epoch_time_arr.append(total_epoch_time)\n",
    "        model_train_accuracy += avg_accuracy\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}, data_loading time: {data_loading_time} \\\n",
    "                compute time: {training_compute_time} total epoch time: {total_epoch_time}\")\n",
    "        print(f\"-------------------------------------------------------------------------------\")\n",
    "        \n",
    "      avg_epoch_time.append({\"lr\":config.lr, \"batch_size\":config.batch_size, \"avg_epoch_time\":sum(epoch_time_arr)/len(epoch_time_arr})\n",
    "      train_accuracy.append({\"lr\":config.lr, \"batch_size\":config.batch_size, \"accuracy\":model_train_accuray/config.epochs})\n",
    "\n",
    "      model.eval()\n",
    "      correct = 0\n",
    "      total = 0\n",
    "    \n",
    "      with torch.no_grad():\n",
    "          for batch in test_loader:\n",
    "              inputs = {\n",
    "                  \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                  \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "              }\n",
    "              labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "              logits = model(**inputs).logits\n",
    "              preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "              correct += (preds == labels).sum().item()\n",
    "              total += labels.size(0)\n",
    "    \n",
    "      accuracy = correct / total\n",
    "      test_accuracy.append({\"lr\":config.lr, \"batch_size\":config.batch_size, \"accuracy\":accuracy})\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607b4e3-a1db-404b-ba17-e9625f62fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        \"optimizer\":[\"SGD\", \"Adam\", \"AdamW\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "wandb.agent(sweep_id, function=train_C7, count=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ec1c41e4f85443d868236d675d4856c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff5b88b3381b4151b68454525b28b50f",
      "placeholder": "​",
      "style": "IPY_MODEL_2e2f639d1d014bb487b918c6425b5745",
      "value": "Map: 100%"
     }
    },
    "2763fccb191a4caba493fc9a544f1cda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e2f639d1d014bb487b918c6425b5745": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71d002ff786f437a9d6c357929e20124": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e60bb5a41184f2cae93e58c6b02ffb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ec1c41e4f85443d868236d675d4856c",
       "IPY_MODEL_fd84667a1aaa44c288d97c0ad517c6d7",
       "IPY_MODEL_b00e56680967401297f32c292ac64e76"
      ],
      "layout": "IPY_MODEL_cae1675924e54ce0826c88b7a2711f24"
     }
    },
    "b00e56680967401297f32c292ac64e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7fe655b8ca94c21be6b7ef837170d7e",
      "placeholder": "​",
      "style": "IPY_MODEL_ce7bffdded0c43439799617ef3e25355",
      "value": " 50000/50000 [00:14&lt;00:00, 3460.34 examples/s]"
     }
    },
    "cae1675924e54ce0826c88b7a2711f24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce7bffdded0c43439799617ef3e25355": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7fe655b8ca94c21be6b7ef837170d7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd84667a1aaa44c288d97c0ad517c6d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71d002ff786f437a9d6c357929e20124",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2763fccb191a4caba493fc9a544f1cda",
      "value": 50000
     }
    },
    "ff5b88b3381b4151b68454525b28b50f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

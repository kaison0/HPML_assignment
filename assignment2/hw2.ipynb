{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "525cb661",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "525cb661",
    "outputId": "4de4e8f5-ffdf-49be-98a3-610b0aeafda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipykernel\n",
      "  Downloading ipykernel-7.0.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting comm>=0.1.1 (from ipykernel)\n",
      "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel)\n",
      "  Downloading debugpy-1.8.17-cp312-cp312-manylinux_2_34_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel)\n",
      "  Downloading ipython-9.6.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting jupyter-client>=8.0.0 (from ipykernel)\n",
      "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
      "  Downloading jupyter_core-5.9.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
      "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting nest-asyncio>=1.4 (from ipykernel)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting packaging>=22 (from ipykernel)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting psutil>=5.7 (from ipykernel)\n",
      "  Downloading psutil-7.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Collecting pyzmq>=25 (from ipykernel)\n",
      "  Downloading pyzmq-27.1.0-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting tornado>=6.2 (from ipykernel)\n",
      "  Downloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting traitlets>=5.4.0 (from ipykernel)\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ipython-pygments-lexers (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack_data (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from jupyter-client>=8.0.0->ipykernel)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
      "  Downloading platformdirs-4.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
      "  Downloading parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
      "  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting executing>=1.2.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pure-eval (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading ipykernel-7.0.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.2/118.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Downloading debugpy-1.8.17-cp312-cp312-manylinux_2_34_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ipython-9.6.0-py3-none-any.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 kB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_core-5.9.1-py3-none-any.whl (29 kB)\n",
      "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading psutil-7.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzmq-27.1.0-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (840 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.0/841.0 kB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.4/391.4 kB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
      "Downloading parso-0.8.5-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pure-eval, ptyprocess, wcwidth, traitlets, tornado, six, pyzmq, pygments, psutil, platformdirs, pexpect, parso, packaging, nest-asyncio, executing, decorator, debugpy, comm, asttokens, stack_data, python-dateutil, prompt_toolkit, matplotlib-inline, jupyter-core, jedi, ipython-pygments-lexers, jupyter-client, ipython, ipykernel\n",
      "Successfully installed asttokens-3.0.0 comm-0.2.3 debugpy-1.8.17 decorator-5.2.1 executing-2.2.1 ipykernel-7.0.1 ipython-9.6.0 ipython-pygments-lexers-1.1.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.9.1 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 packaging-25.0 parso-0.8.5 pexpect-4.9.0 platformdirs-4.5.0 prompt_toolkit-3.0.52 psutil-7.1.1 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.2 python-dateutil-2.9.0.post0 pyzmq-27.1.0 six-1.17.0 stack_data-0.6.3 tornado-6.5.2 traitlets-5.14.3 wcwidth-0.2.14\n",
      "Installed kernelspec myenv in /root/.local/share/jupyter/kernels/myenv\n",
      "Requirement already satisfied: datasets in /venv/main/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: wandb in /venv/main/lib/python3.12/site-packages (0.22.2)\n",
      "Requirement already satisfied: transformers in /venv/main/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /venv/main/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /venv/main/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /venv/main/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: click>=8.0.1 in /venv/main/lib/python3.12/site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /venv/main/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /venv/main/lib/python3.12/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /venv/main/lib/python3.12/site-packages (from wandb) (6.33.0)\n",
      "Requirement already satisfied: pydantic<3 in /venv/main/lib/python3.12/site-packages (from wandb) (2.12.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /venv/main/lib/python3.12/site-packages (from wandb) (2.42.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /venv/main/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /venv/main/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /venv/main/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from transformers) (2025.10.22)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /venv/main/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /venv/main/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /venv/main/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /venv/main/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m venv venv\n",
    "!venv/bin/python -m pip install ipykernel\n",
    "!venv/bin/python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n",
    "%pip install datasets wandb transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ANKbTyr50m9f",
   "metadata": {
    "id": "ANKbTyr50m9f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "torch.cuda.manual_seed_all(423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "K5CiE3dGktBd",
   "metadata": {
    "id": "K5CiE3dGktBd"
   },
   "outputs": [],
   "source": [
    "project_name = \"hpml-hw2-llm2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4u7Bhja-hrwT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4u7Bhja-hrwT",
    "outputId": "a0cd6ab9-d2e6-458a-9aa5-bfa1c782d5a6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728953c5f5874b6fb23546cf52071728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118aee4d402e409c929f7c88e09e7866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6dc25bb8ea4beeb70e231f9d33fe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1089fc5fba43bca93fe8afe97634dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72db3f7519e349e18a403fc393f8df28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cc59c4aa424f3cabb322d93f756ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44caed5059f8442786fa3dd32b310ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "xWy8gotth8SB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWy8gotth8SB",
    "outputId": "2ad07fbc-c752-47bd-8b74-664145fa2469"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e46b928f4604eedb2c836c8149cf264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e68ce9483fb4d38b2086d2c64b7ab7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "-4AMEVJIjjsA",
   "metadata": {
    "id": "-4AMEVJIjjsA"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f0c80669bd4b89833d0b1068b66ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b7fd52-b43a-4cd4-9996-60ebd51f2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "NUMBER_WORKERS = 4\n",
    "train_loader = DataLoader(tokenized[\"train\"], batch_size= BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=NUMBER_WORKERS)\n",
    "test_loader = DataLoader(tokenized[\"test\"], batch_size = BATCH_SIZE, collate_fn=data_collator, num_workers = NUMBER_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yMDy5uS927OD",
   "metadata": {
    "id": "yMDy5uS927OD"
   },
   "source": [
    "## C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "IresskYBi_0H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "IresskYBi_0H",
    "outputId": "aa955777-8b53-46a8-a9e9-89222e30fafc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs32_lr0.0001</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/gk2nv6f6' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/gk2nv6f6</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_033911-gk2nv6f6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_034058-q3kl9vz6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/q3kl9vz6' target=\"_blank\">bs32_lr0.0001</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/q3kl9vz6' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/q3kl9vz6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"hpml-hw2-llm\", name=f\"bs{BATCH_SIZE}_lr{LEARNING_RATE}\", group = \"Warm Up Experiment\")\n",
    "\n",
    "wandb.config.update({\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"num_workers\": NUMBER_WORKERS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"compile_mode\": False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ZsK9vsmTkPpW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsK9vsmTkPpW",
    "outputId": "1ebaa8b3-6f50-4eb0-fb78-368b734158a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 18.16it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0365, Train Accuracy: 0.9878, Test Accuracy: 0.8652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.95it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0248, Train Accuracy: 0.9922, Test Accuracy: 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.84it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0195, Train Accuracy: 0.9944, Test Accuracy: 0.8399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.80it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0201, Train Accuracy: 0.9937, Test Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 782/782 [00:43<00:00, 17.79it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0181, Train Accuracy: 0.9943, Test Accuracy: 0.8186\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  for batch in tqdm(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      inputs = {\n",
    "          \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "          \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          \"labels\": batch[\"labels\"].to(device),\n",
    "      }\n",
    "\n",
    "      outputs = model(**inputs)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      logits = outputs.logits\n",
    "      preds = torch.argmax(logits, dim = 1)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "      correct = (preds == labels).sum().item()\n",
    "      total_correct += correct\n",
    "      total_samples += len(labels)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "  avg_loss = total_loss / len(train_loader)\n",
    "  avg_accuracy = total_correct / total_samples\n",
    "  train_loss.append(avg_loss)\n",
    "  train_accuracy.append(avg_accuracy)\n",
    "\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for batch in test_loader:\n",
    "          inputs = {\n",
    "              \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "              \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          }\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          logits = model(**inputs).logits\n",
    "          preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "          correct += (preds == labels).sum().item()\n",
    "          total += labels.size(0)\n",
    "\n",
    "  accuracy = correct / total\n",
    "  test_accuracy.append(accuracy)\n",
    "\n",
    "  wandb.log({\"train/loss\": train_loss,\n",
    "           \"train/acc\": avg_accuracy,\n",
    "           \"test/acc\": accuracy})\n",
    "  print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LnQpctb_mJnG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "LnQpctb_mJnG",
    "outputId": "1f71f27b-1104-414b-c7da-bcd53cc1defe"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RIoifO1-23Mo",
   "metadata": {
    "id": "RIoifO1-23Mo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sOdFWGecmKdQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "sOdFWGecmKdQ",
    "outputId": "6e92e593-690d-4642-a360-f7ebddc0c3fd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylabel('Value')\n",
    "plt.title('Figure F1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nl2C8CKt4DcD",
   "metadata": {
    "id": "nl2C8CKt4DcD"
   },
   "source": [
    "### Table T1\n",
    "\n",
    "| Epoch | Train Loss | Train Acc | Test Acc |\n",
    "|-------|------------|-----------|----------|\n",
    "|1|0.0317|0.9902|0.8764|\n",
    "|2|0.0246|0.9920|0.8895|\n",
    "|3|0.0211|0.9932|0.8765|\n",
    "|4|0.0178|0.9940|0.8791|\n",
    "|5|0.0173|0.9947|0.8740|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bnMKrcmd56xs",
   "metadata": {
    "id": "bnMKrcmd56xs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gB0_oon5nMFq",
   "metadata": {
    "id": "gB0_oon5nMFq"
   },
   "source": [
    "# C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "kFQIx5zJnNfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "kFQIx5zJnNfa",
    "outputId": "ad0bdcfc-473a-45ab-c768-946d80f0cad4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/acc</td><td>█▄▄▄▁</td></tr><tr><td>train/acc</td><td>▁▆█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/acc</td><td>0.81856</td></tr><tr><td>train/acc</td><td>0.99428</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs32_lr0.0001</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/q3kl9vz6' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/q3kl9vz6</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_034058-q3kl9vz6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_041648-vmpm7rbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/vmpm7rbg' target=\"_blank\">bs32_lr0.0001</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/vmpm7rbg' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm/runs/vmpm7rbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/main/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/venv/main/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/venv/main/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/wandb_run.py\", line 308, in check_stop_status\n",
      "    self._loop_check_status(\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/wandb_run.py\", line 238, in _loop_check_status\n",
      "    local_handle = request()\n",
      "                   ^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/interface/interface.py\", line 992, in deliver_stop_status\n",
      "    return self._deliver_stop_status(status)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py\", line 442, in _deliver_stop_status\n",
      "    return self._deliver(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py\", line 46, in _deliver\n",
      "    return self._asyncer.run(lambda: self.deliver_async(record))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py\", line 133, in run\n",
      "    future = self._schedule(fn, daemon=False)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py\", line 198, in _schedule\n",
      "    raise AlreadyJoinedError\n",
      "wandb.sdk.lib.asyncio_manager.AlreadyJoinedError\n",
      "Exception in thread IntMsgThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/main/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/venv/main/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/venv/main/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/wandb_run.py\", line 335, in check_internal_messages\n",
      "    self._loop_check_status(\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/wandb_run.py\", line 238, in _loop_check_status\n",
      "    local_handle = request()\n",
      "                   ^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/interface/interface.py\", line 1014, in deliver_internal_messages\n",
      "    return self._deliver_internal_messages(internal_message)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py\", line 461, in _deliver_internal_messages\n",
      "    return self._deliver(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py\", line 46, in _deliver\n",
      "    return self._asyncer.run(lambda: self.deliver_async(record))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py\", line 133, in run\n",
      "    future = self._schedule(fn, daemon=False)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py\", line 198, in _schedule\n",
      "    raise AlreadyJoinedError\n",
      "wandb.sdk.lib.asyncio_manager.AlreadyJoinedError\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"hpml-hw2-llm\", name=f\"bs{BATCH_SIZE}_lr{LEARNING_RATE}\", group = \"Baseline Timing\")\n",
    "\n",
    "wandb.config.update({\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"num_workers\": NUMBER_WORKERS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"compile_mode\": False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0RQB4Z5rncZ7",
   "metadata": {
    "id": "0RQB4Z5rncZ7"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "r7dzAx-jnyxJ",
   "metadata": {
    "id": "r7dzAx-jnyxJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_WORKERS = 4\n",
    "train_loader = DataLoader(tokenized[\"train\"], batch_size= BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=NUMBER_WORKERS)\n",
    "test_loader = DataLoader(tokenized[\"test\"], batch_size = BATCH_SIZE, collate_fn=data_collator, num_workers = NUMBER_WORKERS)\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ztMRieKn2bQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ztMRieKn2bQ",
    "outputId": "e6768a04-bd08-46c5-8f4d-753446f7c941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.3058, Train Accuracy: 0.8690, Test Accuracy: 0.8821, data_loading time: 3.445314645767212           compute time: 40.95216727256775 total epoch time: 44.711804151535034\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "data_loading_time_arr = []\n",
    "compute_time_arr = []\n",
    "epoch_time_arr = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  data_loading_time = 0\n",
    "  training_compute_time = 0\n",
    "  total_epoch_time = 0\n",
    "  start_data_loading = time.time()\n",
    "  start_epoch_time = time.time()\n",
    "  for batch in train_loader:\n",
    "      torch.cuda.synchronize()\n",
    "      end = time.time()\n",
    "      data_loading_time +=  end - start_data_loading\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      inputs = {\n",
    "          \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "          \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          \"labels\": batch[\"labels\"].to(device),\n",
    "      }\n",
    "\n",
    "      start_compute = time.time()\n",
    "      outputs = model(**inputs)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      torch.cuda.synchronize()\n",
    "      end = time.time()\n",
    "      training_compute_time += end - start_compute\n",
    "\n",
    "      logits = outputs.logits\n",
    "      preds = torch.argmax(logits, dim = 1)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "      correct = (preds == labels).sum().item()\n",
    "      total_correct += correct\n",
    "      total_samples += len(labels)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      start_data_loading = time.time()\n",
    "\n",
    "  end = time.time()\n",
    "  total_epoch_time = end - start_epoch_time\n",
    "\n",
    "  avg_loss = total_loss / len(train_loader)\n",
    "  avg_accuracy = total_correct / total_samples\n",
    "  train_loss.append(avg_loss)\n",
    "  train_accuracy.append(avg_accuracy)\n",
    "\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for batch in test_loader:\n",
    "          inputs = {\n",
    "              \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "              \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "          }\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          logits = model(**inputs).logits\n",
    "          preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "          correct += (preds == labels).sum().item()\n",
    "          total += labels.size(0)\n",
    "\n",
    "  accuracy = correct / total\n",
    "  test_accuracy.append(accuracy)\n",
    "\n",
    "  wandb.log({\"train/loss\": train_loss,\n",
    "            \"train/acc\": avg_accuracy,\n",
    "            \"test/acc\": accuracy,\n",
    "             \"data-loading time\": data_loading_time,\n",
    "             \"compute time\": training_compute_time,\n",
    "             \"total epoch time\": total_epoch_time})\n",
    "\n",
    "  data_loading_time_arr.append(data_loading_time)\n",
    "  compute_time_arr.append(training_compute_time)\n",
    "  epoch_time_arr.append(total_epoch_time)\n",
    "  print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}, data_loading time: {data_loading_time} \\\n",
    "          compute time: {training_compute_time} total epoch time: {total_epoch_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tSYJy0kk3ZAQ",
   "metadata": {
    "id": "tSYJy0kk3ZAQ"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jLNbkkTs3Zj-",
   "metadata": {
    "id": "jLNbkkTs3Zj-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "heVz9h_S0O_j",
   "metadata": {
    "id": "heVz9h_S0O_j"
   },
   "source": [
    "# C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ylOAPqx30Qkb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ylOAPqx30Qkb",
    "outputId": "7dd7a589-26bf-4f46-f1ec-030b7104228f"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  with wandb.init(\n",
    "      group=\"DataLoader Performance\",\n",
    "      config={\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"max_len\": 256, \n",
    "        \"optimizer\": \"AdamW\", \n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"epochs\": 5, \n",
    "        \"compile_mode\": False\n",
    "    }) as run:\n",
    "      config = wandb.config\n",
    "      run.name = f\"lr{wandb.config.learning_rate}_bs{wandb.config.batch_size}\"\n",
    "      \n",
    "      train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      model_name = \"distilbert-base-uncased\"\n",
    "      model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "      model.to(device)\n",
    "    \n",
    "      optimizer = getattr(optim, config.optimizer)(model.parameters(), lr=config.learning_rate)\n",
    "      print(f\"---------start train: batch_size({config.batch_size}) lr({config.learning_rate}) num_workers({config.num_workers}) optimizer({config.optimizer})-----\")\n",
    "      train_loss = []\n",
    "      train_accuracy = []\n",
    "      test_accuracy = []\n",
    "      data_loading_time_arr = []\n",
    "      compute_time_arr = []\n",
    "      epoch_time_arr = []\n",
    "    \n",
    "      for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        data_loading_time = 0\n",
    "        training_compute_time = 0\n",
    "        total_epoch_time = 0\n",
    "        start_data_loading = time.time()\n",
    "        start_epoch_time = time.time()\n",
    "        for batch in train_loader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            data_loading_time +=  end - start_data_loading\n",
    "\n",
    "            start_compute = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            training_compute_time += end - start_compute\n",
    "    \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim = 1)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += len(labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            start_data_loading = time.time()\n",
    "    \n",
    "        end = time.time()\n",
    "        total_epoch_time = end - start_epoch_time\n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_accuracy = total_correct / total_samples\n",
    "        train_loss.append(avg_loss)\n",
    "        train_accuracy.append(avg_accuracy)\n",
    "    \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "                logits = model(**inputs).logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "        accuracy = correct / total\n",
    "        test_accuracy.append(accuracy)\n",
    "    \n",
    "        wandb.log({\"train/loss\": train_loss,\n",
    "                  \"train/acc\": avg_accuracy,\n",
    "                  \"test/acc\": accuracy,\n",
    "                  \"data-loading time\": data_loading_time,\n",
    "                  \"compute time\": training_compute_time,\n",
    "                  \"total epoch time\": total_epoch_time})\n",
    "    \n",
    "        data_loading_time_arr.append(data_loading_time)\n",
    "        compute_time_arr.append(training_compute_time)\n",
    "        epoch_time_arr.append(total_epoch_time)\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}, data_loading time: {data_loading_time} \\\n",
    "                compute time: {training_compute_time} total epoch time: {total_epoch_time}\")\n",
    "        print(f\"-------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a152793-ac1f-410f-ab52-536c014e43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: lbv42exo\n",
      "Sweep URL: https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: raa2hle7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_050931-raa2hle7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/raa2hle7' target=\"_blank\">helpful-sweep-1</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/raa2hle7' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/raa2hle7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(0) optimizer(AdamW)-----\n",
      "Epoch 1/5, Loss: 0.2978, Train Accuracy: 0.8769, Test Accuracy: 0.8896, data_loading time: 3.5919899940490723                 compute time: 41.070215940475464 total epoch time: 44.86136269569397\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 2/5, Loss: 0.1656, Train Accuracy: 0.9385, Test Accuracy: 0.8901, data_loading time: 3.472041368484497                 compute time: 41.388070821762085 total epoch time: 45.00368905067444\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 3/5, Loss: 0.0879, Train Accuracy: 0.9703, Test Accuracy: 0.8908, data_loading time: 3.617926836013794                 compute time: 41.62524342536926 total epoch time: 45.40958905220032\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 4/5, Loss: 0.0607, Train Accuracy: 0.9797, Test Accuracy: 0.8755, data_loading time: 3.5956647396087646                 compute time: 41.66605877876282 total epoch time: 45.41943955421448\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0381, Train Accuracy: 0.9876, Test Accuracy: 0.8726, data_loading time: 3.4515249729156494                 compute time: 41.58567929267883 total epoch time: 45.177388429641724\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁▅██▇</td></tr><tr><td>data-loading time</td><td>▇▂█▇▁</td></tr><tr><td>test/acc</td><td>███▂▁</td></tr><tr><td>total epoch time</td><td>▁▃██▅</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.58568</td></tr><tr><td>data-loading time</td><td>3.45152</td></tr><tr><td>test/acc</td><td>0.87264</td></tr><tr><td>total epoch time</td><td>45.17739</td></tr><tr><td>train/acc</td><td>0.98756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0001_bs32</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/raa2hle7' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/raa2hle7</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_050931-raa2hle7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6w6yf6zw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_051457-6w6yf6zw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/6w6yf6zw' target=\"_blank\">crimson-sweep-2</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/6w6yf6zw' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/6w6yf6zw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/5, Loss: 0.3074, Train Accuracy: 0.8690, Test Accuracy: 0.8955, data_loading time: 4.123237133026123                 compute time: 42.04955172538757 total epoch time: 51.743576526641846\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 2/5, Loss: 0.1650, Train Accuracy: 0.9393, Test Accuracy: 0.8947, data_loading time: 4.260812997817993                 compute time: 42.082236766815186 total epoch time: 50.379887342453\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 3/5, Loss: 0.0905, Train Accuracy: 0.9684, Test Accuracy: 0.8842, data_loading time: 4.154982805252075                 compute time: 41.948771953582764 total epoch time: 50.16136312484741\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 4/5, Loss: 0.0523, Train Accuracy: 0.9818, Test Accuracy: 0.8782, data_loading time: 4.060473203659058                 compute time: 41.897706747055054 total epoch time: 49.99108052253723\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0427, Train Accuracy: 0.9852, Test Accuracy: 0.8748, data_loading time: 4.093782663345337                 compute time: 41.87591242790222 total epoch time: 49.95531892776489\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▇█▃▂▁</td></tr><tr><td>data-loading time</td><td>▃█▄▁▂</td></tr><tr><td>test/acc</td><td>██▄▂▁</td></tr><tr><td>total epoch time</td><td>█▃▂▁▁</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.87591</td></tr><tr><td>data-loading time</td><td>4.09378</td></tr><tr><td>test/acc</td><td>0.8748</td></tr><tr><td>total epoch time</td><td>49.95532</td></tr><tr><td>train/acc</td><td>0.98516</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0001_bs32</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/6w6yf6zw' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/6w6yf6zw</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_051457-6w6yf6zw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c7cberqh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_052109-c7cberqh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/c7cberqh' target=\"_blank\">ancient-sweep-3</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/c7cberqh' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/c7cberqh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(4) optimizer(AdamW)-----\n",
      "Epoch 1/5, Loss: 0.3026, Train Accuracy: 0.8719, Test Accuracy: 0.9005, data_loading time: 7.019890785217285                 compute time: 41.849918365478516 total epoch time: 55.809765100479126\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 2/5, Loss: 0.1705, Train Accuracy: 0.9375, Test Accuracy: 0.8885, data_loading time: 7.228554964065552                 compute time: 41.862253189086914 total epoch time: 57.63130521774292\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 3/5, Loss: 0.0912, Train Accuracy: 0.9682, Test Accuracy: 0.8884, data_loading time: 7.156970500946045                 compute time: 41.719016313552856 total epoch time: 57.5475378036499\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 4/5, Loss: 0.0601, Train Accuracy: 0.9793, Test Accuracy: 0.8768, data_loading time: 7.1381165981292725                 compute time: 41.60242772102356 total epoch time: 55.46678400039673\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0380, Train Accuracy: 0.9870, Test Accuracy: 0.8752, data_loading time: 7.164135456085205                 compute time: 41.64370083808899 total epoch time: 57.40517330169678\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>██▄▁▂</td></tr><tr><td>data-loading time</td><td>▁█▆▅▆</td></tr><tr><td>test/acc</td><td>█▅▅▁▁</td></tr><tr><td>total epoch time</td><td>▂██▁▇</td></tr><tr><td>train/acc</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.6437</td></tr><tr><td>data-loading time</td><td>7.16414</td></tr><tr><td>test/acc</td><td>0.8752</td></tr><tr><td>total epoch time</td><td>57.40517</td></tr><tr><td>train/acc</td><td>0.98696</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0001_bs32</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/c7cberqh' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/c7cberqh</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_052109-c7cberqh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ae71dmgx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_052828-ae71dmgx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/ae71dmgx' target=\"_blank\">sparkling-sweep-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/sweeps/lbv42exo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/ae71dmgx' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/hpml-hw2-llm2/runs/ae71dmgx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(8) optimizer(AdamW)-----\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'num_workers': {'values': [0, 2, 4, 8]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "wandb.agent(sweep_id, function=train, count=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b708d1",
   "metadata": {},
   "source": [
    "# C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b695c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 782/782 [00:49<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.6935, Train Accuracy: 0.5051, Test Accuracy: 0.4968\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.profiler as profiler\n",
    "\n",
    "EPOCHS = 1\n",
    "NUMBER_WORKERS = 0\n",
    "train_loader = DataLoader(tokenized[\"train\"], batch_size= BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=NUMBER_WORKERS)\n",
    "test_loader = DataLoader(tokenized[\"test\"], batch_size = BATCH_SIZE, collate_fn=data_collator, num_workers = NUMBER_WORKERS)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "with profiler.profile(\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with profiler.record_function(\"data_loading\"):\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                    \"labels\": batch[\"labels\"].to(device),\n",
    "                }\n",
    "\n",
    "            with profiler.record_function(\"forward\"):\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            with profiler.record_function(\"backward\"):\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "            with profiler.record_function(\"optimizer\"):\n",
    "                optimizer.step()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim = 1)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += len(labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_accuracy = total_correct / total_samples\n",
    "        train_loss.append(avg_loss)\n",
    "        train_accuracy.append(avg_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                logits = model(**inputs).logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        test_accuracy.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== CPU Profiling =====\")\n",
    "\n",
    "print(prof)\n",
    "\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\",  # or \"cpu_time_total\"\n",
    "    row_limit=20\n",
    "))\n",
    "\n",
    "# ✅ Print GPU-focused results\n",
    "print(\"\\n===== GPU Profiling =====\")\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"self_cuda_time_total\",  # or \"cuda_time_total\"\n",
    "    row_limit=20\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762bd39-4758-4888-ad32-8920ced06d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb37d7f2-2ab1-4a21-9f2c-67217a11135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'learning_rate': {'values': [5e-5, 1e-4, 5e-4]}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be9992b8-0577-4d7d-a668-4620dea914c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  with wandb.init(config={\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"max_len\": 256, \n",
    "        \"optimizer\": \"AdamW\", \n",
    "        \"num_workers\": 2,\n",
    "        \"epochs\": 2, \n",
    "        \"compile_mode\": False\n",
    "    }) as run:\n",
    "        \n",
    "      train_loader = DataLoader(tokenized[\"train\"], batch_size=config.batch_size, shuffle=True, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      test_loader = DataLoader(tokenized[\"test\"], batch_size=config.batch_size, collate_fn=data_collator, num_workers=config.num_workers)\n",
    "      model_name = \"distilbert-base-uncased\"\n",
    "      model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "      model.to(device)\n",
    "    \n",
    "      optimizer = getattr(optim, config.optimizer.capitalize())(model.parameters(), lr=config.learning_rate)\n",
    "      print(f\"---------start train: batch_size({config.batch_size}) lr({config.learning_rate}) num_workers({config.num_workers}) optimizer({config.optimizer})-----\")\n",
    "      train_loss = []\n",
    "      train_accuracy = []\n",
    "      test_accuracy = []\n",
    "      data_loading_time_arr = []\n",
    "      compute_time_arr = []\n",
    "      epoch_time_arr = []\n",
    "    \n",
    "      for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        data_loading_time = 0\n",
    "        training_compute_time = 0\n",
    "        total_epoch_time = 0\n",
    "        start_data_loading = time.time()\n",
    "        start_epoch_time = time.time()\n",
    "        for batch in train_loader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            data_loading_time +=  end - start_data_loading\n",
    "\n",
    "            start_compute = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            training_compute_time += end - start_compute\n",
    "    \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim = 1)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += len(labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            start_data_loading = time.time()\n",
    "    \n",
    "        end = time.time()\n",
    "        total_epoch_time = end - start_epoch_time\n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_accuracy = total_correct / total_samples\n",
    "        train_loss.append(avg_loss)\n",
    "        train_accuracy.append(avg_accuracy)\n",
    "    \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "                logits = model(**inputs).logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "    \n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "        accuracy = correct / total\n",
    "        test_accuracy.append(accuracy)\n",
    "    \n",
    "        wandb.log({\"train/loss\": train_loss,\n",
    "                  \"train/acc\": avg_accuracy,\n",
    "                  \"test/acc\": accuracy,\n",
    "                  \"data-loading time\": data_loading_time,\n",
    "                  \"compute time\": training_compute_time,\n",
    "                  \"total epoch time\": total_epoch_time})\n",
    "    \n",
    "        data_loading_time_arr.append(data_loading_time)\n",
    "        compute_time_arr.append(training_compute_time)\n",
    "        epoch_time_arr.append(total_epoch_time)\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {avg_accuracy:.4f}, Test Accuracy: {accuracy:.4f}, data_loading time: {data_loading_time} \\\n",
    "                compute time: {training_compute_time} total epoch time: {total_epoch_time}\")\n",
    "        print(f\"-------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61631ae3-18ea-48fd-9991-72fae38a0b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: y2g0jplp\n",
      "Sweep URL: https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: if0a1s74 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_043735-if0a1s74</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74' target=\"_blank\">prime-sweep-1</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(16) lr(5e-05) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.2927, Train Accuracy: 0.8786, Test Accuracy: 0.8994, data_loading time: 5.044431447982788                 compute time: 46.83597779273987 total epoch time: 56.109216928482056\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1575, Train Accuracy: 0.9416, Test Accuracy: 0.9047, data_loading time: 5.043533086776733                 compute time: 47.03125500679016 total epoch time: 56.2285680770874\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>█▁</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>47.03126</td></tr><tr><td>data-loading time</td><td>5.04353</td></tr><tr><td>test/acc</td><td>0.90472</td></tr><tr><td>total epoch time</td><td>56.22857</td></tr><tr><td>train/acc</td><td>0.94164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-sweep-1</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/if0a1s74</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_043735-if0a1s74/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c4bfiq97 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044023-c4bfiq97</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97' target=\"_blank\">snowy-sweep-2</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(16) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.3244, Train Accuracy: 0.8612, Test Accuracy: 0.8822, data_loading time: 5.050256013870239                 compute time: 47.20144319534302 total epoch time: 56.59880566596985\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1842, Train Accuracy: 0.9322, Test Accuracy: 0.8745, data_loading time: 5.0730016231536865                 compute time: 47.05021905899048 total epoch time: 56.21663761138916\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>█▁</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>█▁</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>47.05022</td></tr><tr><td>data-loading time</td><td>5.073</td></tr><tr><td>test/acc</td><td>0.87452</td></tr><tr><td>total epoch time</td><td>56.21664</td></tr><tr><td>train/acc</td><td>0.93224</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-2</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/c4bfiq97</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044023-c4bfiq97/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: apja8pbi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044314-apja8pbi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi' target=\"_blank\">apricot-sweep-3</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(16) lr(0.0005) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.6958, Train Accuracy: 0.5016, Test Accuracy: 0.5000, data_loading time: 4.986021041870117                 compute time: 47.02136778831482 total epoch time: 56.25733828544617\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.6936, Train Accuracy: 0.4960, Test Accuracy: 0.5000, data_loading time: 4.996425151824951                 compute time: 46.96302270889282 total epoch time: 56.07941198348999\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>█▁</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁▁</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>46.96302</td></tr><tr><td>data-loading time</td><td>4.99643</td></tr><tr><td>test/acc</td><td>0.5</td></tr><tr><td>total epoch time</td><td>56.07941</td></tr><tr><td>train/acc</td><td>0.49596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-sweep-3</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/apja8pbi</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044314-apja8pbi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: soqd5vn4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044602-soqd5vn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4' target=\"_blank\">efficient-sweep-4</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(5e-05) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.2873, Train Accuracy: 0.8787, Test Accuracy: 0.8747, data_loading time: 4.104927062988281                 compute time: 41.81464457511902 total epoch time: 50.214582443237305\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1558, Train Accuracy: 0.9434, Test Accuracy: 0.9068, data_loading time: 4.095701456069946                 compute time: 41.877474308013916 total epoch time: 50.101362228393555\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>█▁</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.87747</td></tr><tr><td>data-loading time</td><td>4.0957</td></tr><tr><td>test/acc</td><td>0.9068</td></tr><tr><td>total epoch time</td><td>50.10136</td></tr><tr><td>train/acc</td><td>0.9434</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-4</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/soqd5vn4</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044602-soqd5vn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a8v7clkd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_044839-a8v7clkd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd' target=\"_blank\">vibrant-sweep-5</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.3021, Train Accuracy: 0.8704, Test Accuracy: 0.8973, data_loading time: 4.023552179336548                 compute time: 41.88974690437317 total epoch time: 50.070762634277344\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1693, Train Accuracy: 0.9370, Test Accuracy: 0.8988, data_loading time: 4.086701393127441                 compute time: 42.00715637207031 total epoch time: 50.11471486091614\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>42.00716</td></tr><tr><td>data-loading time</td><td>4.0867</td></tr><tr><td>test/acc</td><td>0.8988</td></tr><tr><td>total epoch time</td><td>50.11471</td></tr><tr><td>train/acc</td><td>0.93696</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sweep-5</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/a8v7clkd</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_044839-a8v7clkd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7yt0t72f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045113-7yt0t72f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f' target=\"_blank\">desert-sweep-6</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(32) lr(0.0005) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.6983, Train Accuracy: 0.5016, Test Accuracy: 0.5000, data_loading time: 4.027059078216553                 compute time: 41.73872375488281 total epoch time: 50.55894684791565\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.6932, Train Accuracy: 0.4984, Test Accuracy: 0.5000, data_loading time: 4.3440101146698                 compute time: 41.72826957702637 total epoch time: 50.1086859703064\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>█▁</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁▁</td></tr><tr><td>total epoch time</td><td>█▁</td></tr><tr><td>train/acc</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>41.72827</td></tr><tr><td>data-loading time</td><td>4.34401</td></tr><tr><td>test/acc</td><td>0.5</td></tr><tr><td>total epoch time</td><td>50.10869</td></tr><tr><td>train/acc</td><td>0.49836</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">desert-sweep-6</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/7yt0t72f</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_045113-7yt0t72f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ehbzvjpl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045344-ehbzvjpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl' target=\"_blank\">stellar-sweep-7</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(64) lr(5e-05) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.2931, Train Accuracy: 0.8748, Test Accuracy: 0.9079, data_loading time: 3.631897449493408                 compute time: 40.23127055168152 total epoch time: 47.82063055038452\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1632, Train Accuracy: 0.9385, Test Accuracy: 0.9058, data_loading time: 3.6717135906219482                 compute time: 40.30282783508301 total epoch time: 48.06329941749573\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>█▁</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>40.30283</td></tr><tr><td>data-loading time</td><td>3.67171</td></tr><tr><td>test/acc</td><td>0.90584</td></tr><tr><td>total epoch time</td><td>48.0633</td></tr><tr><td>train/acc</td><td>0.93848</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-7</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/ehbzvjpl</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_045344-ehbzvjpl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 52db4wm9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045612-52db4wm9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9' target=\"_blank\">polar-sweep-8</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(64) lr(0.0001) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.3137, Train Accuracy: 0.8618, Test Accuracy: 0.8916, data_loading time: 3.6058061122894287                 compute time: 40.00300621986389 total epoch time: 47.626259088516235\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1613, Train Accuracy: 0.9400, Test Accuracy: 0.9052, data_loading time: 3.7015445232391357                 compute time: 40.06726408004761 total epoch time: 47.8132266998291\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>▁█</td></tr><tr><td>data-loading time</td><td>▁█</td></tr><tr><td>test/acc</td><td>▁█</td></tr><tr><td>total epoch time</td><td>▁█</td></tr><tr><td>train/acc</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>compute time</td><td>40.06726</td></tr><tr><td>data-loading time</td><td>3.70154</td></tr><tr><td>test/acc</td><td>0.90516</td></tr><tr><td>total epoch time</td><td>47.81323</td></tr><tr><td>train/acc</td><td>0.93996</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-8</strong> at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/52db4wm9</a><br> View project at: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_045612-52db4wm9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9fm34824 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251021_045835-9fm34824</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/9fm34824' target=\"_blank\">eager-sweep-9</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/sweeps/y2g0jplp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/9fm34824' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/Hyperparameter%20Sensitivity/runs/9fm34824</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------start train: batch_size(64) lr(0.0005) num_workers(2) optimizer(AdamW)-----\n",
      "Epoch 1/2, Loss: 0.6971, Train Accuracy: 0.5002, Test Accuracy: 0.5000, data_loading time: 3.5461485385894775                 compute time: 39.772443771362305 total epoch time: 47.27660632133484\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1984/2333432887.py\", line 36, in train\n",
      "    for batch in train_loader:\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1492, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1454, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1285, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/main/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_1984/2333432887.py\", line 36, in train\n",
      "    for batch in train_loader:\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1492, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1454, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1285, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/main/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name, group=\"Hyperparameter Sensitivity\")\n",
    "\n",
    "wandb.agent(sweep_id, function=train, count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee77f-1105-400a-944d-e071a4fac36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ec1c41e4f85443d868236d675d4856c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff5b88b3381b4151b68454525b28b50f",
      "placeholder": "​",
      "style": "IPY_MODEL_2e2f639d1d014bb487b918c6425b5745",
      "value": "Map: 100%"
     }
    },
    "2763fccb191a4caba493fc9a544f1cda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e2f639d1d014bb487b918c6425b5745": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71d002ff786f437a9d6c357929e20124": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e60bb5a41184f2cae93e58c6b02ffb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ec1c41e4f85443d868236d675d4856c",
       "IPY_MODEL_fd84667a1aaa44c288d97c0ad517c6d7",
       "IPY_MODEL_b00e56680967401297f32c292ac64e76"
      ],
      "layout": "IPY_MODEL_cae1675924e54ce0826c88b7a2711f24"
     }
    },
    "b00e56680967401297f32c292ac64e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7fe655b8ca94c21be6b7ef837170d7e",
      "placeholder": "​",
      "style": "IPY_MODEL_ce7bffdded0c43439799617ef3e25355",
      "value": " 50000/50000 [00:14&lt;00:00, 3460.34 examples/s]"
     }
    },
    "cae1675924e54ce0826c88b7a2711f24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce7bffdded0c43439799617ef3e25355": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7fe655b8ca94c21be6b7ef837170d7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd84667a1aaa44c288d97c0ad517c6d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71d002ff786f437a9d6c357929e20124",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2763fccb191a4caba493fc9a544f1cda",
      "value": 50000
     }
    },
    "ff5b88b3381b4151b68454525b28b50f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
